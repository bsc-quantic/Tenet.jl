var documenterSearchIndex = {"docs":
[{"location":"references.html#References","page":"References","title":"References","text":"","category":"section"},{"location":"references.html","page":"References","title":"References","text":"<div class=\"citation canonical\"><ul><li>\n<div id=\"arute2019quantum\">Arute, F.; Arya, K.; Babbush, R.; Bacon, D.; Bardin, J. C.; Barends, R.; Biswas, R.; Boixo, S.; Brandao, F. G.; Buell, D. A. and others, (2019). <i>Quantum supremacy using a programmable superconducting processor</i>. Nature <b>574</b>, 505â€“510.</div>\n</li><li>\n<div id=\"boixo2018characterizing\">Boixo, S.; Isakov, S. V.; Smelyanskiy, V. N.; Babbush, R.; Ding, N.; Jiang, Z.; Bremner, M. J.; Martinis, J. M. and Neven, H. (2018). <i>Characterizing quantum supremacy in near-term devices</i>. Nature Physics <b>14</b>, 595â€“600.</div>\n</li><li>\n<div id=\"itensor\">Fishman, M.; White, S. R. and Stoudenmire, E. M. (2022). <a href='https://scipost.org/10.21468/SciPostPhysCodeb.4'><i>The ITensor Software Library for Tensor Network Calculations</i></a>. <a href='https://doi.org/10.21468/SciPostPhysCodeb.4'>SciPost Phys. Codebases, 4</a>.</div>\n</li><li>\n<div id=\"gray2018quimb\">Gray, J. (2018), <i>quimb: A python package for quantum information and many-body calculations</i>. Journal of Open Source Software <b>3</b>, 819.</div>\n</li><li>\n<div id=\"cotengra\">Gray, J. (2021), <a href='https://github.com/jcmgray/cotengra'><i>cotengra: Hyper optimized contraction trees for large tensor networks and einsums</i></a>, https://github.com/jcmgray/cotengra.</div>\n</li><li>\n<div id=\"gray2021hyper\">Gray, J. and Kourtis, S. (2021). <i>Hyper-optimized tensor network contraction</i>. Quantum <b>5</b>, 410.</div>\n</li><li>\n<div id=\"hauschild2021tensor\">Hauschild, J.; Pollmann, F. and Zaletel, M. (2021). <i>The Tensor Network Python (TeNPy) Library</i>. In: APS March Meeting Abstracts, R21â€“006.</div>\n</li><li>\n<div id=\"markov2018quantum\">Markov, I. L.; Fatima, A.; Isakov, S. V. and Boixo, S. (2018). <i>Quantum supremacy is both closer and farther than it appears</i>, arXiv preprint arXiv:1807.10749.</div>\n</li><li>\n<div id=\"ramon2023tensorkrowch\">RamÃ³n Pareja Monturiol, J.; PÃ©rez-GarcÃ­a, D. and Pozas-Kerstjens, A. (2023). <i>TensorKrowch: Smooth integration of tensor networks in machine learning</i>, arXiv e-prints, arXivâ€“2306.</div>\n</li><li>\n<div id=\"villalonga2020establishing\">Villalonga, B.; Lyakh, D.; Boixo, S.; Neven, H.; Humble, T. S.; Biswas, R.; Rieffel, E. G.; Ho, A. and Mandr{\\`a}, S. (2020). <i>Establishing the quantum supremacy frontier with a 281 pflop/s simulation</i>. Quantum Science and Technology <b>5</b>, 034003.</div>\n</li>\n</ul></div>","category":"page"},{"location":"visualization.html#Visualization","page":"Visualization","title":"Visualization","text":"","category":"section"},{"location":"visualization.html","page":"Visualization","title":"Visualization","text":"using Makie\nMakie.inline!(true)\nset_theme!(resolution=(800,400))\n\nusing CairoMakie\nCairoMakie.activate!(type = \"svg\")\n\nusing Tenet","category":"page"},{"location":"visualization.html","page":"Visualization","title":"Visualization","text":"Tenet provides a Package Extension for Makie support. You can just import a Makie backend and call Makie.plot on a TensorNetwork.","category":"page"},{"location":"visualization.html","page":"Visualization","title":"Visualization","text":"Makie.plot(::Tenet.TensorNetwork)","category":"page"},{"location":"visualization.html#MakieCore.plot-Tuple{TensorNetwork}","page":"Visualization","title":"MakieCore.plot","text":"plot(tn::TensorNetwork; kwargs...)\nplot!(f::Union{Figure,GridPosition}, tn::TensorNetwork; kwargs...)\nplot!(ax::Union{Axis,Axis3}, tn::TensorNetwork; kwargs...)\n\nPlot a TensorNetwork as a graph.\n\nKeyword Arguments\n\nlabels If true, show the labels of the tensor indices. Defaults to false.\nThe rest of kwargs are passed to GraphMakie.graphplot.\n\n\n\n\n\n","category":"method"},{"location":"visualization.html","page":"Visualization","title":"Visualization","text":"tn = rand(TensorNetwork, 14, 4, seed=0) # hide\nplot(tn, labels=true)","category":"page"},{"location":"examples/ad-tn.html#Automatic-Differentiation-on-Tensor-Network-contraction","page":"Automatic Differentiation on Tensor Network contraction","title":"Automatic Differentiation on Tensor Network contraction","text":"","category":"section"},{"location":"examples/ad-tn.html","page":"Automatic Differentiation on Tensor Network contraction","title":"Automatic Differentiation on Tensor Network contraction","text":"using CairoMakie","category":"page"},{"location":"examples/ad-tn.html","page":"Automatic Differentiation on Tensor Network contraction","title":"Automatic Differentiation on Tensor Network contraction","text":"Tensor Networks have recently gained popularity for Machine Learning tasks. In this example, we show how to perform Automatic Differentiation on Tensor Network contraction to overlap the overlap between two Matrix Product States (MPS) with a smaller dimension.","category":"page"},{"location":"examples/ad-tn.html","page":"Automatic Differentiation on Tensor Network contraction","title":"Automatic Differentiation on Tensor Network contraction","text":"using Tenet\nusing Zygote\nusing Random: seed! # hide\n\nrng = seed!(4) # hide\n\nÏˆ = rand(MPS{Open}, n = 4, p = 2, Ï‡ = 2)\nÏ• = rand(MPS{Open}, n = 4, p = 2, Ï‡ = 4)\nÏˆ = rand(rng, MPS{Open}, n = 4, p = 2, Ï‡ = 2) # hide\nÏ• = rand(rng, MPS{Open}, n = 4, p = 2, Ï‡ = 4) # hide\n\ntn = hcat(Ïˆ, Ï•)\n\nplot(tn) # hide","category":"page"},{"location":"examples/ad-tn.html","page":"Automatic Differentiation on Tensor Network contraction","title":"Automatic Differentiation on Tensor Network contraction","text":"This problem is known as MPS compression. While there are better methods for this matter, this example excels for its simplicity and it can easily be modified for ML tasks. The loss function minimizes when the overlap between the two states psi and phi maximizes, constrained to normalized states.","category":"page"},{"location":"examples/ad-tn.html","page":"Automatic Differentiation on Tensor Network contraction","title":"Automatic Differentiation on Tensor Network contraction","text":"beginaligned\nmin_psi quad  left(braketphi  psi - 1right)^2 \ntextrmst quad  lVert psi rVert^2 = braketpsi mid psi = 1 \n  lVert phi rVert^2 = braketphi mid phi = 1\nendaligned","category":"page"},{"location":"examples/ad-tn.html","page":"Automatic Differentiation on Tensor Network contraction","title":"Automatic Differentiation on Tensor Network contraction","text":"warning: Implicit parameters\nCurrently, calling Zygote.gradient/Zygote.jacobian on functions with explicit parameters doesn't interact well with Tenet data-structures (i.e. Tensor and TensorNetwork) on the interface.While the problem persists, use implicit parameters with Zygote.Params on the arrays (i.e. call Params([parent(tensor)]) or Params([arrays(tensor_network)])).","category":"page"},{"location":"examples/ad-tn.html","page":"Automatic Differentiation on Tensor Network contraction","title":"Automatic Differentiation on Tensor Network contraction","text":"Î· = 0.01\n@time losses = map(1:200) do it\n    # compute gradient\n    loss, âˆ‡ = withgradient(Params(arrays(Ïˆ))) do\n        ((contract(tn) |> first) - 1)^2\n    end\n\n    # direct gradient descent\n    for array in arrays(Ïˆ)\n        array .-= Î· * âˆ‡[array]\n    end\n\n    # normalize state\n    normalize!(Ïˆ)\n\n    return loss\nend\n\nf = Figure() # hide\nax = Axis(f[1, 1], yscale = log10, xscale = identity, xlabel=\"Iterations\") # hide\nlines!(losses, label=\"Loss\") # hide\nlines!(map(x -> 1 - sqrt(x), losses), label=\"Overlap\") # hide\nf[1,2] = Legend(f, ax, framevisible=false) # hide\nf # hide","category":"page"},{"location":"contraction.html#Contraction","page":"Contraction","title":"Contraction","text":"","category":"section"},{"location":"contraction.html","page":"Contraction","title":"Contraction","text":"Contraction path optimization and execution is delegated to the EinExprs library. A EinExpr is a lower-level form of a Tensor Network, in which the contraction path has been laid out as a tree. It is similar to a symbolic expression (i.e. Expr) but in which every node represents an Einstein summation expression (aka einsum).","category":"page"},{"location":"contraction.html","page":"Contraction","title":"Contraction","text":"einexpr(::TensorNetwork)\ncontract(::TensorNetwork)\ncontract!","category":"page"},{"location":"contraction.html#EinExprs.einexpr-Tuple{TensorNetwork}","page":"Contraction","title":"EinExprs.einexpr","text":"einexpr(tn::TensorNetwork; optimizer = EinExprs.Greedy, output = inds(tn, :open), kwargs...)\n\nSearch a contraction path for the given TensorNetwork and return it as a EinExpr.\n\nKeyword Arguments\n\noptimizer Contraction path optimizer. Check EinExprs documentation for more info.\noutputs Indices that won't be contracted. Defaults to open indices.\nkwargs Options to be passed to the optimizer.\n\nSee also: contract.\n\n\n\n\n\n","category":"method"},{"location":"contraction.html#Tenet.contract-Tuple{TensorNetwork}","page":"Contraction","title":"Tenet.contract","text":"contract(tn::TensorNetwork; kwargs...)\n\nContract a TensorNetwork. The contraction order will be first computed by einexpr.\n\nThe kwargs will be passed down to the einexpr function.\n\nSee also: einexpr, contract!.\n\n\n\n\n\n","category":"method"},{"location":"contraction.html#Tenet.contract!","page":"Contraction","title":"Tenet.contract!","text":"contract!(tn::TensorNetwork, index)\n\nIn-place contraction of tensors connected to index.\n\nSee also: contract.\n\n\n\n\n\n","category":"function"},{"location":"examples/google-rqc.html#Google's-Quantum-Advantage-experiment","page":"Google's Quantum Advantage experiment","title":"Google's Quantum Advantage experiment","text":"","category":"section"},{"location":"examples/google-rqc.html","page":"Google's Quantum Advantage experiment","title":"Google's Quantum Advantage experiment","text":"using CairoMakie\nCairoMakie.activate!(type = \"svg\")\nusing NetworkLayout","category":"page"},{"location":"examples/google-rqc.html","page":"Google's Quantum Advantage experiment","title":"Google's Quantum Advantage experiment","text":"info: Dependencies ðŸ“¦\nThis example uses QuacIO and EinExprs in combination with Tenet. Both packages can be found in Quantic's registry and can be installed in Pkg mode.add QuacIO EinExprsIt also requires the circuit in sycamore_m53_d10.qasm file that can be found in here. This is a shorter version of the real circuit used for the experiment.","category":"page"},{"location":"examples/google-rqc.html","page":"Google's Quantum Advantage experiment","title":"Google's Quantum Advantage experiment","text":"In 2019, Google rushed to claim quantum advantage[supremacy] for the first time ever (Arute *et al.*, 2019)(Villalonga *et al.*, 2020). The article was highly criticized and one year later, it was disproved (Gray and Kourtis, 2021) by developing a better heuristic search for contraction path which provided a times 10^4 speedup.","category":"page"},{"location":"examples/google-rqc.html","page":"Google's Quantum Advantage experiment","title":"Google's Quantum Advantage experiment","text":"[supremacy]: The first used term was quantum supremacy although the community transitioned to quantum advantage due to political reasons. However, Google now uses the term beyond classical. It is then not uncommon to find different terms to refer to the same thing: the moment in which quantum computers surpass classical computers on solving some problem.","category":"page"},{"location":"examples/google-rqc.html","page":"Google's Quantum Advantage experiment","title":"Google's Quantum Advantage experiment","text":"Since then, several teams and companies have come and go, proposing and disproving several experiments. But in this example, we focus on the original Google experiment.","category":"page"},{"location":"examples/google-rqc.html","page":"Google's Quantum Advantage experiment","title":"Google's Quantum Advantage experiment","text":"In short, the experiment consisted on sampling Random Quantum Circuits (RQC). The state of the systems after these circuits follow a distribution similar, but not equal to the uniform distribution. Due to noise and decoherence, the fidelity of quantum chips decrease with the circuit depth. The complexity of contracting tensor networks grows with the circuit depth, but due to the fidelity of the physical realization being small, a very rough approximation can be used. In the case of Google, they used tensor slicing for projecting some expensive to contract indices. Since the contribution of each quantum path is guessed to be similar, each slice should contribute a similar part, and by taking the same percentage of slices as the fidelity of the quantum experiment, we obtain a result with a similar fidelity. If you want to read more on the topic, check out (Boixo *et al.*, 2018),(Markov *et al.*, 2018).","category":"page"},{"location":"examples/google-rqc.html","page":"Google's Quantum Advantage experiment","title":"Google's Quantum Advantage experiment","text":"Thanks to Tenet's much cared design, the experiment can be replicated conceptually in less than 20loc.","category":"page"},{"location":"examples/google-rqc.html","page":"Google's Quantum Advantage experiment","title":"Google's Quantum Advantage experiment","text":"using QuacIO\nusing Tenet\n\n_sites = [5, 6, 14, 15, 16, 17, 24, 25, 26, 27, 28, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 61, 62, 63, 64, 65, 66, 67, 72, 73, 74, 75, 76, 83, 84, 85, 94];\n\n# load circuit and convert to `TensorNetwork`\ncircuit = QuacIO.parse(joinpath(@__DIR__, \"sycamore_53_10_0.qasm\"), format = QuacIO.Qflex(), sites = _sites);\ntn = TensorNetwork(circuit)\ntn = view(tn, [i => 1 for i in inds(tn, set=:open)]...)\nplot(tn) # hide","category":"page"},{"location":"examples/google-rqc.html","page":"Google's Quantum Advantage experiment","title":"Google's Quantum Advantage experiment","text":"In order to aid the contraction path optimization, we shrink the search space by using local transformations.","category":"page"},{"location":"examples/google-rqc.html","page":"Google's Quantum Advantage experiment","title":"Google's Quantum Advantage experiment","text":"# simplify Tensor Network by preemptively contracting trivial cases\ntn = transform(tn, Tenet.RankSimplification)\nplot(tn, layout=Stress()) # hide","category":"page"},{"location":"examples/google-rqc.html","page":"Google's Quantum Advantage experiment","title":"Google's Quantum Advantage experiment","text":"Contraction path optimization is the focus of the EinExprs package. For this example, we will use the Greedy algorithm which doesn't yield the optimal path but it's fast and reproducible.","category":"page"},{"location":"examples/google-rqc.html","page":"Google's Quantum Advantage experiment","title":"Google's Quantum Advantage experiment","text":"using EinExprs\npath = einexpr(tn, optimizer = Greedy)\nplot(path, layout=Stress()) # hide","category":"page"},{"location":"examples/google-rqc.html","page":"Google's Quantum Advantage experiment","title":"Google's Quantum Advantage experiment","text":"Then, the indices to be sliced have to be selected. EinExprs provides us with the findslices algorithm (based in the SliceFinder algorithm of (Gray, 2021)) to suggest candidate indices for slicing.","category":"page"},{"location":"examples/google-rqc.html","page":"Google's Quantum Advantage experiment","title":"Google's Quantum Advantage experiment","text":"cuttings = [[i => dim for dim in 1:size(tn,i)] for i in findslices(FlopsScorer(), path, slices=100)]","category":"page"},{"location":"examples/google-rqc.html","page":"Google's Quantum Advantage experiment","title":"Google's Quantum Advantage experiment","text":"Finally, the contraction of slices is parallelized using distributed workers and each contribution is summed to result.","category":"page"},{"location":"examples/google-rqc.html","page":"Google's Quantum Advantage experiment","title":"Google's Quantum Advantage experiment","text":"using Distributed\nusing Iterators: product\n\naddprocs(10)\n\n@everywhere using Tenet, EinExprs\n@everywhere tn = $tn\n@everywhere path = $path\n\nresult = @distributed (+) for proj_inds in product(cuttings...)\n    slice = view(tn, proj_inds...)\n\n    for indices in contractorder(path)\n        contract!(slice, indices)\n    end\n\n    tensors(slice) |> only\nend","category":"page"},{"location":"quantum/index.html#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"quantum/index.html","page":"Introduction","title":"Introduction","text":"In Tenet, we define a Quantum Tensor Network as a TensorNetwork with a notion of sites and directionality.","category":"page"},{"location":"quantum/index.html","page":"Introduction","title":"Introduction","text":"Quantum","category":"page"},{"location":"quantum/index.html#Tenet.Quantum","page":"Introduction","title":"Tenet.Quantum","text":"Quantum <: Ansatz\n\nTensor Network Ansatz that has a notion of sites and directionality (input/output).\n\n\n\n\n\n","category":"type"},{"location":"quantum/index.html","page":"Introduction","title":"Introduction","text":"plug","category":"page"},{"location":"quantum/index.html#Tenet.plug","page":"Introduction","title":"Tenet.plug","text":"plug(::TensorNetwork{<:Quantum})\nplug(::Type{<:TensorNetwork})\n\nReturn the Plug type of the TensorNetwork. The following Plugs are defined in Tenet:\n\nState Only outputs.\nOperator Inputs and outputs.\nProperty No inputs nor outputs.\n\n\n\n\n\n","category":"function"},{"location":"quantum/index.html","page":"Introduction","title":"Introduction","text":"sites","category":"page"},{"location":"quantum/index.html#Tenet.sites","page":"Introduction","title":"Tenet.sites","text":"sites(tn::TensorNetwork{<:Quantum})\n\nReturn the sites in which the TensorNetwork acts.\n\n\n\n\n\n","category":"function"},{"location":"quantum/index.html","page":"Introduction","title":"Introduction","text":"tensors(::TensorNetwork{<:Quantum}, ::Integer)","category":"page"},{"location":"quantum/index.html#Tenet.tensors-Tuple{TensorNetwork{<:Quantum}, Integer}","page":"Introduction","title":"Tenet.tensors","text":"tensors(tn::TensorNetwork{<:Quantum}, site::Integer)\n\nReturn the Tensor connected to the TensorNetwork on site.\n\nSee also: sites.\n\n\n\n\n\n","category":"method"},{"location":"quantum/index.html","page":"Introduction","title":"Introduction","text":"boundary","category":"page"},{"location":"quantum/index.html#Tenet.boundary","page":"Introduction","title":"Tenet.boundary","text":"boundary(::TensorNetwork)\nboundary(::Type{<:TensorNetwork})\n\nReturn the Boundary type of the TensorNetwork. The following Boundarys are defined in Tenet:\n\nOpen\nPeriodic\nInfinite\n\n\n\n\n\n","category":"function"},{"location":"quantum/index.html#Adjoint","page":"Introduction","title":"Adjoint","text":"","category":"section"},{"location":"quantum/index.html","page":"Introduction","title":"Introduction","text":"adjoint","category":"page"},{"location":"quantum/index.html#Base.adjoint","page":"Introduction","title":"Base.adjoint","text":"adjoint(tn::TensorNetwork{<:Quantum})\n\nReturn the adjoint TensorNetwork.\n\nImplementation details\n\nThe tensors are not transposed, just conj! is applied to them.\n\n\n\n\n\n","category":"function"},{"location":"quantum/index.html#Concatenation","page":"Introduction","title":"Concatenation","text":"","category":"section"},{"location":"quantum/index.html","page":"Introduction","title":"Introduction","text":"hcat(::TensorNetwork{<:Quantum}, ::TensorNetwork{<:Quantum})","category":"page"},{"location":"quantum/index.html#Base.hcat-Tuple{TensorNetwork{<:Quantum}, TensorNetwork{<:Quantum}}","page":"Introduction","title":"Base.hcat","text":"hcat(A::TensorNetwork{<:Quantum}, B::TensorNetwork{<:Quantum}...)::TensorNetwork{<:Composite}\n\nJoin TensorNetworks into one by matching sites.\n\n\n\n\n\n","category":"method"},{"location":"quantum/index.html#Norm","page":"Introduction","title":"Norm","text":"","category":"section"},{"location":"quantum/index.html","page":"Introduction","title":"Introduction","text":"LinearAlgebra.norm(::TensorNetwork{<:Quantum}, p::Real)\nLinearAlgebra.normalize!(::TensorNetwork{<:Quantum}, ::Real)","category":"page"},{"location":"quantum/index.html#LinearAlgebra.norm-Tuple{TensorNetwork{<:Quantum}, Real}","page":"Introduction","title":"LinearAlgebra.norm","text":"norm(Ïˆ::TensorNetwork{<:Quantum}, p::Real=2)\n\nCompute the p-norm of a Quantum TensorNetwork.\n\nSee also: normalize!.\n\n\n\n\n\n","category":"method"},{"location":"quantum/index.html#LinearAlgebra.normalize!-Tuple{TensorNetwork{<:Quantum}, Real}","page":"Introduction","title":"LinearAlgebra.normalize!","text":"normalize!(Ïˆ::TensorNetwork{<:Quantum}, p::Real = 2; insert::Union{Nothing,Int} = nothing)\n\nIn-place normalize the TensorNetwork.\n\nKeyword Arguments\n\ninsert Choose the way the normalization is performed:\nIf insert=nothing (default), then all tensors are divided by sqrtnlVert psi rVert_p where n is the number of tensors.\nIf insert isa Integer, then the tensor connected to the site pointed by insert is divided by the norm.\nBoth approaches are mathematically equivalent. Choose between them depending on the numerical properties.\n\nSee also: norm.\n\n\n\n\n\n","category":"method"},{"location":"quantum/index.html#Fidelity","page":"Introduction","title":"Fidelity","text":"","category":"section"},{"location":"quantum/index.html","page":"Introduction","title":"Introduction","text":"fidelity","category":"page"},{"location":"quantum/index.html#Tenet.fidelity","page":"Introduction","title":"Tenet.fidelity","text":"fidelity(Ïˆ,Ï•)\n\nCompute the fidelity between states ketpsi and ketphi.\n\n\n\n\n\n","category":"function"},{"location":"transformations.html#Transformations","page":"Transformations","title":"Transformations","text":"","category":"section"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"using Makie\nMakie.inline!(true)\n\nusing CairoMakie\nusing Tenet\nusing NetworkLayout\n\nfunction smooth_annotation!(f; color=Makie.RGBAf(110 // 256, 170 // 256, 250 // 256, 60 // 256), xlims=[-2, 2], ylims=[-2, 2], offset_x=0.0, offset_y=0.0, radius_x=1.0, radius_y=1.0, num_waves=5, fluctuation_amplitude=0.1, phase_shift=0.0)\n    ax = Axis(f)\n    hidedecorations!(ax)\n    hidespines!(ax)\n\n    # Define limits of the plot\n    xlims!(ax, xlims...)\n    ylims!(ax, ylims...)\n\n    # Create a perturbed filled shape\n    theta = LinRange(0, 2Ï€, 100)\n\n    fluctuations = fluctuation_amplitude .* sin.(num_waves .* theta .+ phase_shift)\n\n    # Apply the fluctuations and radius scaling\n    perturbed_radius_x = radius_x .+ fluctuations\n    perturbed_radius_y = radius_y .+ fluctuations\n\n    circle_points = [Point2f((perturbed_radius_x[i]) * cos(theta[i]) + offset_x,\n                              (perturbed_radius_y[i]) * sin(theta[i]) + offset_y) for i in 1:length(theta)]\n\n    poly!(ax, circle_points, color=color, closed=true)\nend\n\nbg_blue = Makie.RGBAf(110 // 256, 170 // 256, 250 // 256, 50 // 256)\norange = Makie.RGBf(240 // 256, 180 // 256, 100 // 256)\nred = Makie.RGBf(240 // 256, 90 // 256, 70 // 256)","category":"page"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"In tensor network computations, it is good practice to apply various transformations to simplify the network structure, reduce computational cost, or prepare the network for further operations. These transformations modify the network's structure locally by permuting, contracting, factoring or truncating tensors.","category":"page"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"A crucial reason why these methods are indispensable lies in their ability to drastically reduce the problem size of the contraction path search and also the contraction. This doesn't necessarily involve reducing the maximum rank of the Tensor Network itself, but more importantly, it reduces the size (or rank) of the involved tensors.","category":"page"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"Our approach is based in (Gray and Kourtis, 2021), which can also be found in quimb.","category":"page"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"In Tenet, we provide a set of predefined transformations which you can apply to your TensorNetwork using both the transform/transform! functions.","category":"page"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"transform\ntransform!","category":"page"},{"location":"transformations.html#Tenet.transform","page":"Transformations","title":"Tenet.transform","text":"transform(tn::TensorNetwork, config::Transformation)\ntransform(tn::TensorNetwork, configs)\n\nReturn a new TensorNetwork where some Transformation has been performed into it.\n\nSee also: transform!.\n\n\n\n\n\n","category":"function"},{"location":"transformations.html#Tenet.transform!","page":"Transformations","title":"Tenet.transform!","text":"transform!(tn::TensorNetwork, config::Transformation)\ntransform!(tn::TensorNetwork, configs)\n\nIn-place version of transform.\n\n\n\n\n\n","category":"function"},{"location":"transformations.html#Transformations-2","page":"Transformations","title":"Transformations","text":"","category":"section"},{"location":"transformations.html#Hyperindex-converter","page":"Transformations","title":"Hyperindex converter","text":"","category":"section"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"Tenet.HyperindConverter","category":"page"},{"location":"transformations.html#Tenet.HyperindConverter","page":"Transformations","title":"Tenet.HyperindConverter","text":"HyperindConverter <: Transformation\n\nConvert hyperindices to COPY-tensors, represented by DeltaArrays. This transformation is always used by default when visualizing a TensorNetwork with plot.\n\n\n\n\n\n","category":"type"},{"location":"transformations.html#Diagonal-reduction","page":"Transformations","title":"Diagonal reduction","text":"","category":"section"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"Tenet.DiagonalReduction","category":"page"},{"location":"transformations.html#Tenet.DiagonalReduction","page":"Transformations","title":"Tenet.DiagonalReduction","text":"DiagonalReduction <: Transformation\n\nReduce the dimension of a Tensor in a TensorNetwork when it has a pair of indices that fulfil a diagonal structure.\n\nKeyword Arguments\n\natol Absolute tolerance. Defaults to 1e-12.\n\n\n\n\n\n","category":"type"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"set_theme!(resolution=(800,200)) # hide\nfig = Figure() #hide\n\ndata = zeros(Float64, 2, 2, 2, 2) #hide\nfor i in 1:2 #hide\n    for j in 1:2 #hide\n        for k in 1:2 #hide\n            data[i, i, j, k] = k #hide\n        end #hide\n    end #hide\nend #hide\n\nA = Tensor(data, (:i, :j, :k, :l)) #hide\nB = Tensor(rand(2, 2), (:i, :m)) #hide\nC = Tensor(rand(2, 2), (:j, :n)) #hide\n\ntn = TensorNetwork([A, B, C]) #hide\nreduced = transform(tn, Tenet.DiagonalReduction) #hide\n\nsmooth_annotation!( #hide\n    fig[1, 1]; #hide\n    color = bg_blue, #hide\n    xlims = [-2, 2], #hide\n    ylims = [-2, 2], #hide\n    offset_x = -0.21, #hide\n    offset_y = -0.42, #hide\n    radius_x = 0.38, #hide\n    radius_y = 0.8, #hide\n    num_waves = 6, #hide\n    fluctuation_amplitude = 0.02, #hide\n    phase_shift = 0.0) #hide\nplot!(fig[1, 1], tn, layout=Spring(iterations=1000, C=0.5, seed=100); node_color=[red, orange, orange, :black, :black,:black, :black]) #hide\n\nsmooth_annotation!( #hide\n    fig[1, 2]; #hide\n    color = bg_blue, #hide\n    xlims = [-2, 2], #hide\n    ylims = [-2, 2], #hide\n    offset_x = 0.1, #hide\n    offset_y = -0.35, #hide\n    radius_x = 0.38, #hide\n    radius_y = 1.1, #hide\n    num_waves = 5, #hide\n    fluctuation_amplitude = 0.02, #hide\n    phase_shift = 1.9) #hide\nplot!(fig[1, 2], reduced, layout=Spring(iterations=1000, C=0.5, seed=100),  node_color=[orange, orange, red, :black, :black, :black, :black, :black]) #hide\n\nLabel(fig[1, 1, Bottom()], \"Original\") #hide\nLabel(fig[1, 2, Bottom()], \"Transformed\") #hide\n\nfig #hide","category":"page"},{"location":"transformations.html#Anti-diagonal-reduction","page":"Transformations","title":"Anti-diagonal reduction","text":"","category":"section"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"Tenet.AntiDiagonalGauging","category":"page"},{"location":"transformations.html#Tenet.AntiDiagonalGauging","page":"Transformations","title":"Tenet.AntiDiagonalGauging","text":"AntiDiagonalGauging <: Transformation\n\nReverse the order of tensor indices that fulfill the anti-diagonal condition. While this transformation doesn't directly enhance computational efficiency, it sets up the TensorNetwork for other operations that do.\n\nKeyword Arguments\n\natol Absolute tolerance. Defaults to 1e-12.\nskip List of indices to skip. Defaults to [].\n\n\n\n\n\n","category":"type"},{"location":"transformations.html#Rank-simplification","page":"Transformations","title":"Rank simplification","text":"","category":"section"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"Tenet.RankSimplification","category":"page"},{"location":"transformations.html#Tenet.RankSimplification","page":"Transformations","title":"Tenet.RankSimplification","text":"RankSimplification <: Transformation\n\nPreemptively contract tensors whose result doesn't increase in size.\n\n\n\n\n\n","category":"type"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"set_theme!(resolution=(800,200)) # hide\nfig = Figure() #hide\n\nA = Tensor(rand(2, 2, 2, 2), (:i, :j, :k, :l)) #hide\nB = Tensor(rand(2, 2), (:i, :m)) #hide\nC = Tensor(rand(2, 2, 2), (:m, :n, :o)) #hide\nE = Tensor(rand(2, 2, 2, 2), (:o, :p, :q, :j)) #hide\n\ntn = TensorNetwork([A, B, C, E]) #hide\nreduced = transform(tn, Tenet.RankSimplification) #hide\n\nsmooth_annotation!( #hide\n    fig[1, 1]; #hide\n    color = bg_blue, #hide\n    xlims = [-2, 2], #hide\n    ylims = [-2, 2], #hide\n    offset_x = -0.32, #hide\n    offset_y = -0.5, #hide\n    radius_x = 0.25, #hide\n    radius_y = 0.94, #hide\n    num_waves = 6, #hide\n    fluctuation_amplitude = 0.01, #hide\n    phase_shift = 0.0) #hide\nplot!(fig[1, 1], tn, layout=Spring(iterations=1000, C=0.5, seed=20); node_color=[orange, red, orange, orange, :black, :black, :black, :black, :black]) #hide\n\nsmooth_annotation!( #hide\n    fig[1, 2]; #hide\n    color = bg_blue, #hide\n    xlims = [-2, 2], #hide\n    ylims = [-2, 2], #hide\n    offset_x = 0.12, #hide\n    offset_y = -0.62, #hide\n    radius_x = 0.18, #hide\n    radius_y = 0.46, #hide\n    num_waves = 5, #hide\n    fluctuation_amplitude = 0.01, #hide\n    phase_shift = 0) #hide\nplot!(fig[1, 2], reduced, layout=Spring(iterations=1000, C=0.5, seed=1); node_color=[red, orange, orange, :black, :black, :black, :black, :black]) #hide\n\nLabel(fig[1, 1, Bottom()], \"Original\") #hide\nLabel(fig[1, 2, Bottom()], \"Transformed\") #hide\n\nfig #hide","category":"page"},{"location":"transformations.html#Column-reduction","page":"Transformations","title":"Column reduction","text":"","category":"section"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"Tenet.ColumnReduction","category":"page"},{"location":"transformations.html#Tenet.ColumnReduction","page":"Transformations","title":"Tenet.ColumnReduction","text":"ColumnReduction <: Transformation\n\nTruncate the dimension of a Tensor in a TensorNetwork when it contains columns with all elements smaller than atol.\n\nKeyword Arguments\n\natol Absolute tolerance. Defaults to 1e-12.\nskip List of indices to skip. Defaults to [].\n\n\n\n\n\n","category":"type"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"set_theme!(resolution=(800,200)) # hide\nfig = Figure() #hide\n\ndata = rand(3, 3, 3) #hide\ndata[:, 1:2, :] .= 0 #hide\n\nA = Tensor(data, (:i, :j, :k)) #hide\nB = Tensor(rand(3, 3), (:j, :l)) #hide\nC = Tensor(rand(3, 3), (:l, :m)) #hide\n\ntn = TensorNetwork([A, B, C]) #hide\nreduced = transform(tn, Tenet.ColumnReduction) #hide\n\nsmooth_annotation!( #hide\n    fig[1, 1]; #hide\n    color = bg_blue, #hide\n    xlims = [-2, 2], #hide\n    ylims = [-2, 2], #hide\n    offset_x = -1.12, #hide\n    offset_y = -0.22, #hide\n    radius_x = 0.35, #hide\n    radius_y = 0.84, #hide\n    num_waves = 4, #hide\n    fluctuation_amplitude = 0.02, #hide\n    phase_shift = 0.0) #hide\nplot!(fig[1, 1], tn, layout=Spring(iterations=1000, C=0.5, seed=6); node_color=[red, orange, orange, :black, :black, :black]) #hide\n\nsmooth_annotation!( #hide\n    fig[1, 2]; #hide\n    color = bg_blue, #hide\n    xlims = [-2, 2], #hide\n    ylims = [-2, 2], #hide\n    offset_x = -0.64, #hide\n    offset_y = 1.2, #hide\n    radius_x = 0.32, #hide\n    radius_y = 0.78, #hide\n    num_waves = 5, #hide\n    fluctuation_amplitude = 0.02, #hide\n    phase_shift = 0) #hide\n\nLabel(fig[1, 1, Bottom()], \"Original\") #hide\nLabel(fig[1, 2, Bottom()], \"Transformed\") #hide\nplot!(fig[1, 2], reduced, layout=Spring(iterations=2000, C=40, seed=8); node_color=[red, orange, orange, :black, :black, :black]) #hide\n\nfig #hide","category":"page"},{"location":"transformations.html#Split-simplification","page":"Transformations","title":"Split simplification","text":"","category":"section"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"Tenet.SplitSimplification","category":"page"},{"location":"transformations.html#Tenet.SplitSimplification","page":"Transformations","title":"Tenet.SplitSimplification","text":"SplitSimplification <: Transformation\n\nReduce the rank of tensors in the TensorNetwork by decomposing them using the Singular Value Decomposition (SVD). Tensors whose factorization do not increase the maximum rank of the network are left decomposed.\n\nKeyword Arguments\n\natol Absolute tolerance. Defaults to 1e-10.\n\n\n\n\n\n","category":"type"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"set_theme!(resolution=(800,200)) # hide\nfig = Figure() #hide\n\nv1 = Tensor([1, 2, 3], (:i,)) #hide\nv2 = Tensor([4, 5, 6], (:j,)) #hide\nm1 = Tensor(rand(3, 3), (:k, :l)) #hide\n\nt1 = contract(v1, v2) #hide\ntensor = contract(t1, m1)  #hide\n\ntn = TensorNetwork([tensor, Tensor(rand(3, 3, 3), (:k, :m, :n)), Tensor(rand(3, 3, 3), (:l, :n, :o))]) #hide\nreduced = transform(tn, Tenet.SplitSimplification) #hide\n\nsmooth_annotation!( #hide\n    fig[1, 1]; #hide\n    color = bg_blue, #hide\n    xlims = [-2, 2], #hide\n    ylims = [-2, 2], #hide\n    offset_x = 0.24, #hide\n    offset_y = 0.6, #hide\n    radius_x = 0.32, #hide\n    radius_y = 0.78, #hide\n    num_waves = 5, #hide\n    fluctuation_amplitude = 0.015, #hide\n    phase_shift = 0.0) #hide\nplot!(fig[1, 1], tn, layout=Spring(iterations=10000, C=0.5, seed=12); node_color=[red, orange,  orange, :black, :black, :black, :black]) #hide\n\nsmooth_annotation!( #hide\n    fig[1, 2]; #hide\n    color = bg_blue, #hide\n    xlims = [-2, 2], #hide\n    ylims = [-2, 2], #hide\n    offset_x = -0.2, #hide\n    offset_y = -0.4, #hide\n    radius_x = 1.1, #hide\n    radius_y = 0.75, #hide\n    num_waves = 3, #hide\n    fluctuation_amplitude = 0.18, #hide\n    phase_shift = 0.8) #hide\n\nLabel(fig[1, 1, Bottom()], \"Original\") #hide\nLabel(fig[1, 2, Bottom()], \"Transformed\") #hide\nplot!(fig[1, 2], reduced, layout=Spring(iterations=10000, C=13, seed=151); node_color=[orange, orange, red, red, red, :black, :black, :black, :black]) #hide\n\nfig #hide","category":"page"},{"location":"transformations.html#Example:-RQC-simplification","page":"Transformations","title":"Example: RQC simplification","text":"","category":"section"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"Local transformations can dramatically reduce the complexity of tensor networks. Take as an example the Random Quantum Circuit circuit on the Sycamore chip from Google's quantum advantage experiment (Arute *et al.*, 2019).","category":"page"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"using QuacIO\nset_theme!(resolution=(800,400)) # hide\n\nsites = [5, 6, 14, 15, 16, 17, 24, 25, 26, 27, 28, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 61, 62, 63, 64, 65, 66, 67, 72, 73, 74, 75, 76, 83, 84, 85, 94]\ncircuit = QuacIO.parse(joinpath(@__DIR__, \"sycamore_53_10_0.qasm\"), format=QuacIO.Qflex(), sites=sites)\ntn = TensorNetwork(circuit)\n\n# Apply transformations to the tensor network\ntransformed_tn = transform(tn, [Tenet.AntiDiagonalGauging, Tenet.DiagonalReduction, Tenet.ColumnReduction, Tenet.RankSimplification])\n\nfig = Figure() # hide\nax1 = Axis(fig[1, 1]) # hide\np1 = plot!(ax1, tn; edge_width=0.75, node_size=8., node_attr=(strokecolor=:black, strokewidth=0.5)) # hide\nax2 = Axis(fig[1, 2]) # hide\np2 = plot!(ax2, transformed_tn; edge_width=0.75, node_size=8., node_attr=(strokecolor=:black, strokewidth=0.5)) # hide\nax1.titlesize, ax2.titlesize = 20, 20 # hide\nhidedecorations!(ax1) # hide\nhidespines!(ax1) # hide\nhidedecorations!(ax2) # hide\nhidespines!(ax2) # hide\n\nLabel(fig[1, 1, Bottom()], \"Original\") # hide\nLabel(fig[1, 2, Bottom()], \"Transformed\") # hide\n\nfig # hide","category":"page"},{"location":"quantum/mps.html#Matrix-Product-States-(MPS)","page":"Matrix Product States (MPS)","title":"Matrix Product States (MPS)","text":"","category":"section"},{"location":"quantum/mps.html","page":"Matrix Product States (MPS)","title":"Matrix Product States (MPS)","text":"Matrix Product States (MPS) are a Quantum Tensor Network ansatz whose tensors are laid out in a 1D chain. Due to this, these networks are also known as Tensor Trains in other mathematical fields. Depending on the boundary conditions, the chains can be open or closed (i.e. periodic boundary conditions).","category":"page"},{"location":"quantum/mps.html","page":"Matrix Product States (MPS)","title":"Matrix Product States (MPS)","text":"using Makie\nMakie.inline!(true)\nset_theme!(resolution=(800,200))\n\nusing CairoMakie\n\nusing Tenet\nusing NetworkLayout","category":"page"},{"location":"quantum/mps.html","page":"Matrix Product States (MPS)","title":"Matrix Product States (MPS)","text":"fig = Figure() # hide\n\ntn_open = rand(MatrixProduct{State,Open}, n=10, Ï‡=4) # hide\ntn_periodic = rand(MatrixProduct{State,Periodic}, n=10, Ï‡=4) # hide\n\nplot!(fig[1,1], tn_open, layout=Spring(iterations=1000, C=0.5, seed=100)) # hide\nplot!(fig[1,2], tn_periodic, layout=Spring(iterations=1000, C=0.5, seed=100)) # hide\n\nLabel(fig[1,1, Bottom()], \"Open\") # hide\nLabel(fig[1,2, Bottom()], \"Periodic\") # hide\n\nfig # hide","category":"page"},{"location":"quantum/mps.html#Matrix-Product-Operators-(MPO)","page":"Matrix Product States (MPS)","title":"Matrix Product Operators (MPO)","text":"","category":"section"},{"location":"quantum/mps.html","page":"Matrix Product States (MPS)","title":"Matrix Product States (MPS)","text":"Matrix Product Operators (MPO) are the operator version of Matrix Product State (MPS). The major difference between them is that MPOs have 2 indices per site (1 input and 1 output) while MPSs only have 1 index per site (i.e. an output).","category":"page"},{"location":"quantum/mps.html","page":"Matrix Product States (MPS)","title":"Matrix Product States (MPS)","text":"fig = Figure() # hide\n\ntn_open = rand(MatrixProduct{Operator,Open}, n=10, Ï‡=4) # hide\ntn_periodic = rand(MatrixProduct{Operator,Periodic}, n=10, Ï‡=4) # hide\n\nplot!(fig[1,1], tn_open, layout=Spring(iterations=1000, C=0.5, seed=100)) # hide\nplot!(fig[1,2], tn_periodic, layout=Spring(iterations=1000, C=0.5, seed=100)) # hide\n\nLabel(fig[1,1, Bottom()], \"Open\") # hide\nLabel(fig[1,2, Bottom()], \"Periodic\") # hide\n\nfig # hide","category":"page"},{"location":"quantum/mps.html","page":"Matrix Product States (MPS)","title":"Matrix Product States (MPS)","text":"In Tenet, the generic MatrixProduct ansatz implements this topology. Type variables are used to address their functionality (State or Operator) and their boundary conditions (Open or Periodic).","category":"page"},{"location":"quantum/mps.html","page":"Matrix Product States (MPS)","title":"Matrix Product States (MPS)","text":"MatrixProduct\nMatrixProduct(::Any)","category":"page"},{"location":"quantum/mps.html#Tenet.MatrixProduct","page":"Matrix Product States (MPS)","title":"Tenet.MatrixProduct","text":"MatrixProduct{P<:Plug,B<:Boundary} <: Quantum\n\nA generic ansatz representing Matrix Product State (MPS) and Matrix Product Operator (MPO) topology, aka Tensor Train. Type variable P represents the Plug type (State or Operator) and B represents the Boundary type (Open or Periodic).\n\nAnsatz Fields\n\nÏ‡::Union{Nothing,Int} Maximum virtual bond dimension.\n\n\n\n\n\n","category":"type"},{"location":"quantum/mps.html#Tenet.MatrixProduct-Tuple{Any}","page":"Matrix Product States (MPS)","title":"Tenet.MatrixProduct","text":"MatrixProduct{P,B}(arrays::AbstractArray[]; Ï‡::Union{Nothing,Int} = nothing, order = defaultorder(MatrixProduct{P}))\n\nConstruct a TensorNetwork with MatrixProduct ansatz, from the arrays of the tensors.\n\nKeyword Arguments\n\nÏ‡ Maximum virtual bond dimension. Defaults to nothing.\norder Order of tensor indices on arrays. Defaults to (:l, :r, :o) if P is a State, (:l, :r, :i, :o) if Operator.\n\n\n\n\n\n","category":"method"},{"location":"alternatives.html#Alternatives","page":"Alternatives","title":"Alternatives","text":"","category":"section"},{"location":"alternatives.html","page":"Alternatives","title":"Alternatives","text":"Tenet is strongly opinionated. We acknowledge that it may not suit all cases (although we try ðŸ™‚). If your case doesn't fit Tenet's design, you can try the following libraries:","category":"page"},{"location":"alternatives.html","page":"Alternatives","title":"Alternatives","text":"quimb (Gray, 2018) Flexible Tensor Network written in Python. Main source of inspiration for Tenet.\ntenpy (Hauschild *et al.*, 2021) Tensor Network library written in Python with a strong focus on physics.\nITensors.jl (Fishman *et al.*, 2022) Mature Tensor Network framework written in Julia.\ntensorkrowch (RamÃ³n Pareja Monturiol *et al.*, 2023) A new Tensor Network library built on top of PyTorch.","category":"page"},{"location":"quantum/peps.html#Projected-Entangled-Pair-States-(PEPS)","page":"Projected Entangled Pair States (PEPS)","title":"Projected Entangled Pair States (PEPS)","text":"","category":"section"},{"location":"quantum/peps.html","page":"Projected Entangled Pair States (PEPS)","title":"Projected Entangled Pair States (PEPS)","text":"Projected Entangled Pair States (PEPS) are a Quantum Tensor Network ansatz whose tensors are laid out in a 2D lattice. Depending on the boundary conditions, the chains can be open or closed (i.e. periodic boundary conditions).","category":"page"},{"location":"quantum/peps.html","page":"Projected Entangled Pair States (PEPS)","title":"Projected Entangled Pair States (PEPS)","text":"using Makie\nMakie.inline!(true)\nset_theme!(resolution=(800,400))\n\nusing CairoMakie\nCairoMakie.activate!(type = \"svg\")\n\nusing Tenet\nusing NetworkLayout","category":"page"},{"location":"quantum/peps.html","page":"Projected Entangled Pair States (PEPS)","title":"Projected Entangled Pair States (PEPS)","text":"fig = Figure() # hide\n\ntn_open = rand(PEPS{Open}, rows=10, cols=10, Ï‡=4) # hide\ntn_periodic = rand(PEPS{Periodic}, rows=10, cols=10, Ï‡=4) # hide\n\nplot!(fig[1,1], tn_open, layout=Stress(seed=1)) # hide\nplot!(fig[1,2], tn_periodic, layout=Stress(seed=10,dim=2,iterations=100000)) # hide\n\nLabel(fig[1,1, Bottom()], \"Open\") # hide\nLabel(fig[1,2, Bottom()], \"Periodic\") # hide\n\nfig # hide","category":"page"},{"location":"quantum/peps.html#Projected-Entangled-Pair-Operators-(PEPO)","page":"Projected Entangled Pair States (PEPS)","title":"Projected Entangled Pair Operators (PEPO)","text":"","category":"section"},{"location":"quantum/peps.html","page":"Projected Entangled Pair States (PEPS)","title":"Projected Entangled Pair States (PEPS)","text":"fig = Figure() # hide\n\ntn_open = rand(PEPO{Open}, rows=10, cols=10, Ï‡=4) # hide\ntn_periodic = rand(PEPO{Periodic}, rows=10, cols=10, Ï‡=4) # hide\n\nplot!(fig[1,1], tn_open, layout=Stress(seed=1)) # hide\nplot!(fig[1,2], tn_periodic, layout=Stress(seed=10,dim=2,iterations=100000)) # hide\n\nLabel(fig[1,1, Bottom()], \"Open\") # hide\nLabel(fig[1,2, Bottom()], \"Periodic\") # hide\n\nfig # hide","category":"page"},{"location":"quantum/peps.html","page":"Projected Entangled Pair States (PEPS)","title":"Projected Entangled Pair States (PEPS)","text":"ProjectedEntangledPair\nProjectedEntangledPair(::Any)","category":"page"},{"location":"quantum/peps.html#Tenet.ProjectedEntangledPair","page":"Projected Entangled Pair States (PEPS)","title":"Tenet.ProjectedEntangledPair","text":"ProjectedEntangledPair{P<:Plug,B<:Boundary} <: Quantum\n\nA generic ansatz representing Projected Entangled Pair States (PEPS) and Projected Entangled Pair Operators (PEPO). Type variable P represents the Plug type (State or Operator) and B represents the Boundary type (Open or Periodic).\n\nAnsatz Fields\n\nÏ‡::Union{Nothing,Int} Maximum virtual bond dimension.\n\n\n\n\n\n","category":"type"},{"location":"quantum/peps.html#Tenet.ProjectedEntangledPair-Tuple{Any}","page":"Projected Entangled Pair States (PEPS)","title":"Tenet.ProjectedEntangledPair","text":"ProjectedEntangledPair{P,B}(arrays::Matrix{AbstractArray}; Ï‡::Union{Nothing,Int} = nothing, order = defaultorder(ProjectedEntangledPair{P}))\n\nConstruct a TensorNetwork with ProjectedEntangledPair ansatz, from the arrays of the tensors.\n\nKeyword Arguments\n\nÏ‡ Maximum virtual bond dimension. Defaults to nothing.\norder Order of the tensor indices on arrays. Defaults to (:l, :r, :u, :d, :o) if P is a State, (:l, :r, :u, :d, :i, :o) if Operator.\n\n\n\n\n\n","category":"method"},{"location":"tensors.html#Tensors","page":"Tensors","title":"Tensors","text":"","category":"section"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"using Tenet","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"There are many jokes[1] about how to define a tensor. The definition we are giving here might not be the most correct one, but it is good enough for our use case (don't kill me please, mathematicians). A tensor T of order[2] n is a multilinear[3] application between n vector spaces over a field mathcalF.","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"[1]: For example, recursive definitions like a tensor is whatever that transforms as a tensor.","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"[2]: The order of a tensor may also be known as rank or dimensionality in other fields. However, these can be missleading, since it has nothing to do with the rank of linear algebra nor with the dimensionality of a vector space. We prefer to use word order.","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"[3]: Meaning that the relationships between the output and the inputs, and the inputs between them, are linear.","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"T  mathcalF^dim(1) times dots times mathcalF^dim(n) mapsto mathcalF","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"In layman's terms, it is a linear function whose inputs are vectors and the output is a scalar number.","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"T(mathbfv^(1) dots mathbfv^(n)) = c in mathcalF qquadqquad forall i mathbfv^(i) in mathcalF^dim(i)","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"Tensor algebra is a higher-order generalization of linear algebra, where scalar numbers can be viewed as order-0 tensors, vectors as order-1 tensors, matrices as order-2 tensors, ...","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"<img src=\"assets/tensor.excalidraw.svg\" class=\"invert-on-dark\"/>","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"Letters are used to identify each of the vector spaces the tensor relates to. In computer science, you would intuitively think of tensors as \"n-dimensional arrays with named dimensions\".","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"T_ijk iff mathttTijk","category":"page"},{"location":"tensors.html#The-Tensor-type","page":"Tensors","title":"The Tensor type","text":"","category":"section"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"In Tenet, a tensor is represented by the Tensor type, which wraps an array and a list of symbols. As it subtypes AbstractArray, many array operations can be dispatched to it.","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"You can create a Tensor by passing an array and a list of Symbols that name indices.","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"Táµ¢â±¼â‚– = Tensor(rand(3,5,2), (:i,:j,:k))","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"The dimensionality or size of each index can be consulted using the size function.","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"Base.size(::Tensor)","category":"page"},{"location":"tensors.html#Base.size-Tuple{Tensor}","page":"Tensors","title":"Base.size","text":"Base.size(::Tensor[, i])\n\nReturn the size of the underlying array or the dimension i (specified by Symbol or Integer).\n\n\n\n\n\n","category":"method"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"size(Táµ¢â±¼â‚–)\nsize(Táµ¢â±¼â‚–, :j)\nlength(Táµ¢â±¼â‚–)","category":"page"},{"location":"tensors.html#Operations","page":"Tensors","title":"Operations","text":"","category":"section"},{"location":"tensors.html#Contraction","page":"Tensors","title":"Contraction","text":"","category":"section"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"Tenet.contract(::Tensor, ::Tensor)","category":"page"},{"location":"tensors.html#Tenet.contract-Tuple{Tensor, Tensor}","page":"Tensors","title":"Tenet.contract","text":"contract(a::Tensor[, b::Tensor, dims=nonunique([inds(a)..., inds(b)...])])\n\nPerform tensor contraction operation.\n\n\n\n\n\n","category":"method"},{"location":"index.html#Tenet.jl","page":"Home","title":"Tenet.jl","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"info: BSC-Quantic's Registry\nTenet and some of its dependencies are located in our own Julia registry. In order to download Tenet, add our registry to your Julia installation by using the Pkg mode in a REPL session,using Pkg\npkg\"registry add https://github.com/bsc-quantic/Registry\"","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"A Julia library for Tensor Networks. Tenet can be executed both at local environments and on large supercomputers. Its goals are,","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Expressiveness Simple to use ðŸ‘¶\nFlexibility Extend it to your needs ðŸ”§\nPerformance Goes brr... fast ðŸŽï¸","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"A video of its presentation at JuliaCon 2023 can be seen here:","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"<div class=\"youtube-video\">\n<iframe class=\"youtube-video\" width=\"560\" src=\"https://www.youtube-nocookie.com/embed/8BHGtm6FRMk?si=bPXB6bPtK695HFIR\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n</div>","category":"page"},{"location":"index.html#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Optimized Tensor Network contraction, powered by EinExprs\nTensor Network slicing/cuttings\nAutomatic Differentiation of TN contraction, powered by EinExprs and ChainRules\nQuantum Tensor Networks\nMatrix Product States (MPS)\nMatrix Product Operators (MPO)\nProjected Entangled Pair States (PEPS)\n3D visualization of large networks, powered by Makie\nTranslation from quantum circuits, powered by Quac","category":"page"},{"location":"index.html#Roadmap","page":"Home","title":"Roadmap","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"The following feature are not yet implemented but are work in progress or are thought to be implemented in the near-mid future:","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Distributed contraction\nQuantum Tensor Networks\nTree Tensor Networks (TTN)\nMultiscale Entanglement Renormalization Ansatz (MERA)\nNumerical Tensor Network algorithms\nTensor Renormalization Group (TRG)\nDensity Matrix Renormalization Group (DMRG)","category":"page"},{"location":"tensor-network.html#Tensor-Networks","page":"Introduction","title":"Tensor Networks","text":"","category":"section"},{"location":"tensor-network.html","page":"Introduction","title":"Introduction","text":"Tensor Networks (TN) are a graphical notation for representing complex multi-linear functions. For example, the following equation","category":"page"},{"location":"tensor-network.html","page":"Introduction","title":"Introduction","text":"sum_ijklmnop A_im B_ijp C_njk D_pkl E_mno F_ol","category":"page"},{"location":"tensor-network.html","page":"Introduction","title":"Introduction","text":"can be represented visually as","category":"page"},{"location":"tensor-network.html","page":"Introduction","title":"Introduction","text":"<figure>\n<img width=500 src=\"assets/tn-sketch.svg\" alt=\"Sketch of a Tensor Network\">\n<figcaption>Sketch of a Tensor Network</figcaption>\n</figure>","category":"page"},{"location":"tensor-network.html","page":"Introduction","title":"Introduction","text":"The graph's nodes represent tensors and edges represent tensor indices.","category":"page"},{"location":"tensor-network.html","page":"Introduction","title":"Introduction","text":"In Tenet, these objects are represented by the TensorNetwork type.","category":"page"},{"location":"tensor-network.html","page":"Introduction","title":"Introduction","text":"TensorNetwork","category":"page"},{"location":"tensor-network.html#Tenet.TensorNetwork","page":"Introduction","title":"Tenet.TensorNetwork","text":"TensorNetwork{Ansatz}\n\nGraph of interconnected tensors, representing a multilinear equation. Graph vertices represent tensors and graph edges, tensor indices.\n\n\n\n\n\n","category":"type"},{"location":"tensor-network.html","page":"Introduction","title":"Introduction","text":"Information about a TensorNetwork can be queried with the following functions.","category":"page"},{"location":"tensor-network.html#Query-information","page":"Introduction","title":"Query information","text":"","category":"section"},{"location":"tensor-network.html","page":"Introduction","title":"Introduction","text":"inds(::TensorNetwork)\nsize(::TensorNetwork)\ntensors(::TensorNetwork)\nlength(::TensorNetwork)\nansatz","category":"page"},{"location":"tensor-network.html#EinExprs.inds-Tuple{TensorNetwork}","page":"Introduction","title":"EinExprs.inds","text":"inds(tn::TensorNetwork, set = :all)\n\nReturn the names of the indices in the TensorNetwork.\n\nKeyword Arguments\n\nset\n:all (default) All indices.\n:open Indices only mentioned in one tensor.\n:inner Indices mentioned at least twice.\n:hyper Indices mentioned at least in three tensors.\n\n\n\n\n\n","category":"method"},{"location":"tensor-network.html#Base.size-Tuple{TensorNetwork}","page":"Introduction","title":"Base.size","text":"size(tn::TensorNetwork)\nsize(tn::TensorNetwork, index)\n\nReturn a mapping from indices to their dimensionalities.\n\nIf index is set, return the dimensionality of index. This is equivalent to size(tn)[index].\n\n\n\n\n\n","category":"method"},{"location":"tensor-network.html#Tenet.tensors-Tuple{TensorNetwork}","page":"Introduction","title":"Tenet.tensors","text":"tensors(tn::TensorNetwork)\n\nReturn a list of the Tensors in the TensorNetwork.\n\n\n\n\n\n","category":"method"},{"location":"tensor-network.html#Base.length-Tuple{TensorNetwork}","page":"Introduction","title":"Base.length","text":"length(tn::TensorNetwork)\n\nReturn the number of Tensors in the TensorNetwork.\n\nSee also: tensors, size.\n\n\n\n\n\n","category":"method"},{"location":"tensor-network.html#Tenet.ansatz","page":"Introduction","title":"Tenet.ansatz","text":"ansatz(::TensorNetwork{Ansatz})\nansatz(::Type{<:TensorNetwork{Ansatz}})\n\nReturn the Ansatz of a TensorNetwork type or object.\n\n\n\n\n\n","category":"function"},{"location":"tensor-network.html#Modification","page":"Introduction","title":"Modification","text":"","category":"section"},{"location":"tensor-network.html#Add/Remove-tensors","page":"Introduction","title":"Add/Remove tensors","text":"","category":"section"},{"location":"tensor-network.html","page":"Introduction","title":"Introduction","text":"push!(::TensorNetwork, ::Tensor)\nappend!(::TensorNetwork, ::Base.AbstractVecOrTuple{<:Tensor})\npop!(::TensorNetwork, ::Tensor)\ndelete!(::TensorNetwork, ::Any)","category":"page"},{"location":"tensor-network.html#Base.push!-Tuple{TensorNetwork, Tensor}","page":"Introduction","title":"Base.push!","text":"push!(tn::TensorNetwork, tensor::Tensor)\n\nAdd a new tensor to the Tensor Network.\n\nSee also: append!, pop!.\n\n\n\n\n\n","category":"method"},{"location":"tensor-network.html#Base.append!-Tuple{TensorNetwork, Union{Tuple{Vararg{var\"#s58\"}}, AbstractVector{<:var\"#s58\"}} where var\"#s58\"<:Tensor}","page":"Introduction","title":"Base.append!","text":"append!(tn::TensorNetwork, tensors::AbstractVecOrTuple{<:Tensor})\nappend!(A::TensorNetwork, B::TensorNetwork)\n\nAdd a list of tensors to the first TensorNetwork.\n\nSee also: push!\n\n\n\n\n\n","category":"method"},{"location":"tensor-network.html#Base.pop!-Tuple{TensorNetwork, Tensor}","page":"Introduction","title":"Base.pop!","text":"pop!(tn::TensorNetwork, tensor::Tensor)\npop!(tn::TensorNetwork, i::Union{Symbol,AbstractVecOrTuple{Symbol}})\n\nRemove a tensor from the Tensor Network and returns it. If a Tensor is passed, then the first tensor satisfies egality (i.e. â‰¡ or ===) will be removed. If a Symbol or a list of Symbols is passed, then remove and return the tensors that contain all the indices.\n\nSee also: push!, delete!.\n\n\n\n\n\n","category":"method"},{"location":"tensor-network.html#Base.delete!-Tuple{TensorNetwork, Any}","page":"Introduction","title":"Base.delete!","text":"delete!(tn::TensorNetwork, x)\n\nLike pop! but return the TensorNetwork instead.\n\n\n\n\n\n","category":"method"},{"location":"tensor-network.html#Replace-existing-elements","page":"Introduction","title":"Replace existing elements","text":"","category":"section"},{"location":"tensor-network.html","page":"Introduction","title":"Introduction","text":"replace\nreplace!","category":"page"},{"location":"tensor-network.html#Base.replace","page":"Introduction","title":"Base.replace","text":"replace(tn::TensorNetwork, old => new...)\n\nReturn a copy of the TensorNetwork where old has been replaced by new.\n\nSee also: replace!.\n\n\n\n\n\n","category":"function"},{"location":"tensor-network.html#Base.replace!","page":"Introduction","title":"Base.replace!","text":"replace!(tn::TensorNetwork, old => new...)\n\nReplace the element in old with the one in new. Depending on the types of old and new, the following behaviour is expected:\n\nIf Symbols, it will correspond to a index renaming.\nIf Tensors, first element that satisfies egality (â‰¡ or ===) will be replaced.\n\nSee also: replace.\n\n\n\n\n\n","category":"function"},{"location":"tensor-network.html#Selection","page":"Introduction","title":"Selection","text":"","category":"section"},{"location":"tensor-network.html","page":"Introduction","title":"Introduction","text":"select\nselectdim\nslice!\nview(::TensorNetwork)","category":"page"},{"location":"tensor-network.html#Tenet.select","page":"Introduction","title":"Tenet.select","text":"select(tn::TensorNetwork, i)\n\nReturn tensors whose indices match with the list of indices i.\n\n\n\n\n\n","category":"function"},{"location":"tensor-network.html#Base.selectdim","page":"Introduction","title":"Base.selectdim","text":"selectdim(tn::TensorNetwork, index::Symbol, i)\n\nReturn a copy of the TensorNetwork where index has been projected to dimension i.\n\nSee also: view, slice!.\n\n\n\n\n\n","category":"function"},{"location":"tensor-network.html#Tenet.slice!","page":"Introduction","title":"Tenet.slice!","text":"slice!(tn::TensorNetwork, index::Symbol, i)\n\nIn-place projection of index on dimension i.\n\nSee also: selectdim, view.\n\n\n\n\n\n","category":"function"},{"location":"tensor-network.html#Base.view-Tuple{TensorNetwork}","page":"Introduction","title":"Base.view","text":"view(tn::TensorNetwork, index => i...)\n\nReturn a copy of the TensorNetwork where each index has been projected to dimension i. It is equivalent to a recursive call of selectdim.\n\nSee also: selectdim, slice!.\n\n\n\n\n\n","category":"method"},{"location":"tensor-network.html#Miscelaneous","page":"Introduction","title":"Miscelaneous","text":"","category":"section"},{"location":"tensor-network.html","page":"Introduction","title":"Introduction","text":"Base.copy(::TensorNetwork)\nBase.rand(::Type{TensorNetwork}, n::Integer, regularity::Integer)","category":"page"},{"location":"tensor-network.html#Base.copy-Tuple{TensorNetwork}","page":"Introduction","title":"Base.copy","text":"copy(tn::TensorNetwork)\n\nReturn a shallow copy of the TensorNetwork.\n\n\n\n\n\n","category":"method"},{"location":"tensor-network.html#Base.rand-Tuple{Type{TensorNetwork}, Integer, Integer}","page":"Introduction","title":"Base.rand","text":"rand(TensorNetwork, n::Integer, regularity::Integer; out = 0, dim = 2:9, seed = nothing, globalind = false)\n\nGenerate a random tensor network.\n\nArguments\n\nn Number of tensors.\nregularity Average number of indices per tensor.\nout Number of open indices.\ndim Range of dimension sizes.\nseed If not nothing, seed random generator with this value.\nglobalind Add a global 'broadcast' dimension to every tensor.\n\n\n\n\n\n","category":"method"}]
}
