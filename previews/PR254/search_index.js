var documenterSearchIndex = {"docs":
[{"location":"references.html#References","page":"References","title":"References","text":"","category":"section"},{"location":"references.html","page":"References","title":"References","text":"Fishman, M.; White, S. R. and Stoudenmire, E. M. (2022). The ITensor Software Library for Tensor Network Calculations. SciPost Phys. Codebases, 4.\n\n\n\nGray, J. (2018), quimb: A python package for quantum information and many-body calculations. Journal of Open Source Software 3, 819.\n\n\n\nGray, J. and Kourtis, S. (2021). Hyper-optimized tensor network contraction. Quantum 5, 410.\n\n\n\nHauschild, J.; Pollmann, F. and Zaletel, M. (2021). The Tensor Network Python (TeNPy) Library. In: APS March Meeting Abstracts, Vol. 2021; p. R21–006.\n\n\n\nRamón Pareja Monturiol, J.; Pérez-García, D. and Pozas-Kerstjens, A. (2023). TensorKrowch: Smooth integration of tensor networks in machine learning, arXiv e-prints, arXiv–2306.\n\n\n\n","category":"page"},{"location":"developer/type-hierarchy.html#Inheritance-and-Traits","page":"TensorNetwork type hierarchy","title":"Inheritance and Traits","text":"","category":"section"},{"location":"developer/type-hierarchy.html","page":"TensorNetwork type hierarchy","title":"TensorNetwork type hierarchy","text":"Julia (and in general, all modern languages like Rust or Go) implement Object Oriented Programming (OOP) in a rather restricted form compared to popular OOP languages like Java, C++ or Python. In particular, they forbid structural inheritance; i.e. inheriting fields from parent superclass(es).","category":"page"},{"location":"developer/type-hierarchy.html","page":"TensorNetwork type hierarchy","title":"TensorNetwork type hierarchy","text":"In recent years, structural inheritance has increasingly been considered a bad practice, favouring composition instead.","category":"page"},{"location":"developer/type-hierarchy.html","page":"TensorNetwork type hierarchy","title":"TensorNetwork type hierarchy","text":"Julia design space on this topic is not completely clear. Julia has abstract types, which can be \"inherited\" but do not have fields and can't be instantiated, and concrete types, which cannot be inherited from them but have fields and can be instantiated. In this sense, implementing methods with Julia's abstract types act as some kind of polymorphic base class.","category":"page"},{"location":"developer/type-hierarchy.html","page":"TensorNetwork type hierarchy","title":"TensorNetwork type hierarchy","text":"As of the time of writing, the type hierarchy of Tenet looks like this:","category":"page"},{"location":"developer/type-hierarchy.html","page":"TensorNetwork type hierarchy","title":"TensorNetwork type hierarchy","text":"graph TD\n    id1(AbstractTensorNetwork) --> TensorNetwork\n    id1(AbstractTensorNetwork) --> id2(AbstractQuantum)\n    id2(AbstractQuantum) --> Quantum\n    id2(AbstractQuantum) --> id3(Ansatz)\n    id3(Ansatz) --> Product\n    id3(Ansatz) --> Dense\n    id3(Ansatz) --> Chain\n    style id1 stroke-dasharray: 5 5\n    style id2 stroke-dasharray: 5 5\n    style id3 stroke-dasharray: 5 5","category":"page"},{"location":"visualization.html#Visualization","page":"Visualization","title":"Visualization","text":"","category":"section"},{"location":"visualization.html","page":"Visualization","title":"Visualization","text":"using Makie\nusing GraphMakie\nusing NetworkLayout\n\nMakie.inline!(true)\nset_theme!(resolution=(800,400))\n\nusing CairoMakie\nCairoMakie.activate!(type = \"svg\")\n\nusing Tenet","category":"page"},{"location":"visualization.html","page":"Visualization","title":"Visualization","text":"Tenet provides a Package Extension for Makie support. You can just import a Makie backend and call GraphMakie.graphplot on a TensorNetwork.","category":"page"},{"location":"visualization.html","page":"Visualization","title":"Visualization","text":"GraphMakie.graphplot(::Tenet.TensorNetwork)","category":"page"},{"location":"visualization.html#GraphMakie.graphplot-Tuple{TensorNetwork}","page":"Visualization","title":"GraphMakie.graphplot","text":"graphplot(tn::TensorNetwork; kwargs...)\ngraphplot!(f::Union{Figure,GridPosition}, tn::TensorNetwork; kwargs...)\ngraphplot!(ax::Union{Axis,Axis3}, tn::TensorNetwork; kwargs...)\n\nPlot a TensorNetwork as a graph.\n\nKeyword Arguments\n\nlabels If true, show the labels of the tensor indices. Defaults to false.\nThe rest of kwargs are passed to GraphMakie.graphplot.\n\n\n\n\n\n","category":"method"},{"location":"visualization.html","page":"Visualization","title":"Visualization","text":"tn = rand(TensorNetwork, 14, 4, seed=0) # hide\ngraphplot(tn, layout=Stress(), labels=true)","category":"page"},{"location":"ansatz/product.html#Product-ansatz","page":"Product ansatz","title":"Product ansatz","text":"","category":"section"},{"location":"contraction.html#Contraction","page":"Contraction","title":"Contraction","text":"","category":"section"},{"location":"contraction.html","page":"Contraction","title":"Contraction","text":"Contraction path optimization and execution is delegated to the EinExprs library. A EinExpr is a lower-level form of a Tensor Network, in which the contraction path has been laid out as a tree. It is similar to a symbolic expression (i.e. Expr) but in which every node represents an Einstein summation expression (aka einsum).","category":"page"},{"location":"contraction.html","page":"Contraction","title":"Contraction","text":"einexpr(::Tenet.TensorNetwork)\ncontract(::Tenet.TensorNetwork)\ncontract!","category":"page"},{"location":"contraction.html#EinExprs.einexpr-Tuple{TensorNetwork}","page":"Contraction","title":"EinExprs.einexpr","text":"einexpr(tn::AbstractTensorNetwork; optimizer = EinExprs.Greedy, output = inds(tn, :open), kwargs...)\n\nSearch a contraction path for the given AbstractTensorNetwork and return it as a EinExpr.\n\nKeyword Arguments\n\noptimizer Contraction path optimizer. Check EinExprs documentation for more info.\noutputs Indices that won't be contracted. Defaults to open indices.\nkwargs Options to be passed to the optimizer.\n\nSee also: contract.\n\n\n\n\n\n","category":"method"},{"location":"contraction.html#Tenet.contract!","page":"Contraction","title":"Tenet.contract!","text":"contract!(tn::TensorNetwork, index)\n\nIn-place contraction of tensors connected to index.\n\nSee also: contract.\n\n\n\n\n\n","category":"function"},{"location":"transformations.html#Transformations","page":"Transformations","title":"Transformations","text":"","category":"section"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"using Makie\nMakie.inline!(true)\nusing GraphMakie\nusing CairoMakie\nusing Tenet\nusing NetworkLayout","category":"page"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"In tensor network computations, it is good practice to apply various transformations to simplify the network structure, reduce computational cost, or prepare the network for further operations. These transformations modify the network's structure locally by permuting, contracting, factoring or truncating tensors.","category":"page"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"A crucial reason why these methods are indispensable lies in their ability to drastically reduce the problem size of the contraction path search and also the contraction. This doesn't necessarily involve reducing the maximum rank of the Tensor Network itself, but more importantly, it reduces the size (or rank) of the involved tensors.","category":"page"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"Our approach is based in (Gray and Kourtis, 2021), which can also be found in quimb.","category":"page"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"In Tenet, we provide a set of predefined transformations which you can apply to your TensorNetwork using both the transform/transform! functions.","category":"page"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"transform\ntransform!","category":"page"},{"location":"transformations.html#Tenet.transform","page":"Transformations","title":"Tenet.transform","text":"transform(tn::TensorNetwork, config::Transformation)\ntransform(tn::TensorNetwork, configs)\n\nReturn a new TensorNetwork where some Transformation has been performed into it.\n\nSee also: transform!.\n\n\n\n\n\n","category":"function"},{"location":"transformations.html#Tenet.transform!","page":"Transformations","title":"Tenet.transform!","text":"transform!(tn::TensorNetwork, config::Transformation)\ntransform!(tn::TensorNetwork, configs)\n\nIn-place version of transform.\n\n\n\n\n\n","category":"function"},{"location":"transformations.html#Available-transformations","page":"Transformations","title":"Available transformations","text":"","category":"section"},{"location":"transformations.html#Hyperindex-converter","page":"Transformations","title":"Hyperindex converter","text":"","category":"section"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"Tenet.HyperFlatten\nTenet.HyperGroup","category":"page"},{"location":"transformations.html#Tenet.HyperFlatten","page":"Transformations","title":"Tenet.HyperFlatten","text":"HyperFlatten <: Transformation\n\nConvert hyperindices to COPY-tensors, represented by DeltaArrays. This transformation is always used by default when visualizing a TensorNetwork with plot.\n\nSee also: HyperGroup.\n\n\n\n\n\n","category":"type"},{"location":"transformations.html#Tenet.HyperGroup","page":"Transformations","title":"Tenet.HyperGroup","text":"HyperGroup <: Transformation\n\nConvert COPY-tensors, represented by DeltaArrays, to hyperindices.\n\nSee also: HyperFlatten.\n\n\n\n\n\n","category":"type"},{"location":"transformations.html#Contraction-simplification","page":"Transformations","title":"Contraction simplification","text":"","category":"section"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"Tenet.ContractSimplification","category":"page"},{"location":"transformations.html#Tenet.ContractSimplification","page":"Transformations","title":"Tenet.ContractSimplification","text":"ContractSimplification <: Transformation\n\nPreemptively contract tensors whose result doesn't increase in size.\n\n\n\n\n\n","category":"type"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"set_theme!(resolution=(800,200)) # hide\nfig = Figure() #hide\n\nA = Tensor(rand(2, 2, 2, 2), (:i, :j, :k, :l)) #hide\nB = Tensor(rand(2, 2), (:i, :m)) #hide\nC = Tensor(rand(2, 2, 2), (:m, :n, :o)) #hide\nE = Tensor(rand(2, 2, 2, 2), (:o, :p, :q, :j)) #hide\n\ntn = TensorNetwork([A, B, C, E]) #hide\nreduced = transform(tn, Tenet.ContractSimplification) #hide\n\ngraphplot!(fig[1, 1], tn; layout=Stress(), labels=true) #hide\ngraphplot!(fig[1, 2], reduced; layout=Stress(), labels=true) #hide\n\nLabel(fig[1, 1, Bottom()], \"Original\") #hide\nLabel(fig[1, 2, Bottom()], \"Transformed\") #hide\n\nfig #hide","category":"page"},{"location":"transformations.html#Diagonal-reduction","page":"Transformations","title":"Diagonal reduction","text":"","category":"section"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"Tenet.DiagonalReduction","category":"page"},{"location":"transformations.html#Tenet.DiagonalReduction","page":"Transformations","title":"Tenet.DiagonalReduction","text":"DiagonalReduction <: Transformation\n\nReduce the dimension of a Tensor in a TensorNetwork when it has a pair of indices that fulfil a diagonal structure.\n\nKeyword Arguments\n\natol Absolute tolerance. Defaults to 1e-12.\n\n\n\n\n\n","category":"type"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"set_theme!(resolution=(800,200)) # hide\nfig = Figure() #hide\n\ndata = zeros(Float64, 2, 2, 2, 2) #hide\nfor i in 1:2 #hide\n    for j in 1:2 #hide\n        for k in 1:2 #hide\n            data[i, i, j, k] = k #hide\n        end #hide\n    end #hide\nend #hide\n\nA = Tensor(data, (:i, :j, :k, :l)) #hide\nB = Tensor(rand(2, 2), (:i, :m)) #hide\nC = Tensor(rand(2, 2), (:j, :n)) #hide\n\ntn = TensorNetwork([A, B, C]) #hide\nreduced = transform(tn, Tenet.DiagonalReduction) #hide\n\ngraphplot!(fig[1, 1], tn; layout=Stress(), labels=true) #hide\ngraphplot!(fig[1, 2], reduced; layout=Stress(), labels=true) #hide\n\nLabel(fig[1, 1, Bottom()], \"Original\") #hide\nLabel(fig[1, 2, Bottom()], \"Transformed\") #hide\n\nfig #hide","category":"page"},{"location":"transformations.html#Anti-diagonal-reduction","page":"Transformations","title":"Anti-diagonal reduction","text":"","category":"section"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"Tenet.AntiDiagonalGauging","category":"page"},{"location":"transformations.html#Tenet.AntiDiagonalGauging","page":"Transformations","title":"Tenet.AntiDiagonalGauging","text":"AntiDiagonalGauging <: Transformation\n\nReverse the order of tensor indices that fulfill the anti-diagonal condition. While this transformation doesn't directly enhance computational efficiency, it sets up the TensorNetwork for other operations that do.\n\nKeyword Arguments\n\natol Absolute tolerance. Defaults to 1e-12.\nskip List of indices to skip. Defaults to [].\n\n\n\n\n\n","category":"type"},{"location":"transformations.html#Dimension-truncation","page":"Transformations","title":"Dimension truncation","text":"","category":"section"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"Tenet.Truncate","category":"page"},{"location":"transformations.html#Tenet.Truncate","page":"Transformations","title":"Tenet.Truncate","text":"Truncate <: Transformation\n\nTruncate the dimension of a Tensor in a TensorNetwork when it contains columns with all elements smaller than atol.\n\nKeyword Arguments\n\natol Absolute tolerance. Defaults to 1e-12.\nskip List of indices to skip. Defaults to [].\n\n\n\n\n\n","category":"type"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"set_theme!(resolution=(800,200)) # hide\nfig = Figure() #hide\n\ndata = rand(3, 3, 3) #hide\ndata[:, 1:2, :] .= 0 #hide\n\nA = Tensor(data, (:i, :j, :k)) #hide\nB = Tensor(rand(3, 3), (:j, :l)) #hide\nC = Tensor(rand(3, 3), (:l, :m)) #hide\n\ntn = TensorNetwork([A, B, C]) #hide\nreduced = transform(tn, Tenet.Truncate) #hide\n\ngraphplot!(fig[1, 1], tn; layout=Spring(C=10), labels=true) #hide\ngraphplot!(fig[1, 2], reduced; layout=Spring(C=10), labels=true) #hide\n\nLabel(fig[1, 1, Bottom()], \"Original\") #hide\nLabel(fig[1, 2, Bottom()], \"Transformed\") #hide\n\nfig #hide","category":"page"},{"location":"transformations.html#Split-simplification","page":"Transformations","title":"Split simplification","text":"","category":"section"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"Tenet.SplitSimplification","category":"page"},{"location":"transformations.html#Tenet.SplitSimplification","page":"Transformations","title":"Tenet.SplitSimplification","text":"SplitSimplification <: Transformation\n\nReduce the rank of tensors in the TensorNetwork by decomposing them using the Singular Value Decomposition (SVD). Tensors whose factorization do not increase the maximum rank of the network are left decomposed.\n\nKeyword Arguments\n\natol Absolute tolerance. Defaults to 1e-10.\n\n\n\n\n\n","category":"type"},{"location":"transformations.html","page":"Transformations","title":"Transformations","text":"set_theme!(resolution=(800,200)) # hide\nfig = Figure() #hide\n\nv1 = Tensor([1, 2, 3], (:i,)) #hide\nv2 = Tensor([4, 5, 6], (:j,)) #hide\nm1 = Tensor(rand(3, 3), (:k, :l)) #hide\n\nt1 = contract(v1, v2) #hide\ntensor = contract(t1, m1)  #hide\n\ntn = TensorNetwork([ #hide\n    tensor, #hide\n    Tensor(rand(3, 3, 3), (:k, :m, :n)), #hide\n    Tensor(rand(3, 3, 3), (:l, :n, :o)) #hide\n]) #hide\nreduced = transform(tn, Tenet.SplitSimplification) #hide\n\ngraphplot!(fig[1, 1], tn; layout=Stress(), labels=true) #hide\ngraphplot!(fig[1, 2], reduced, layout=Spring(C=11); labels=true) #hide\n\nLabel(fig[1, 1, Bottom()], \"Original\") #hide\nLabel(fig[1, 2, Bottom()], \"Transformed\") #hide\n\nfig #hide","category":"page"},{"location":"ansatz/chain.html#Matrix-Product-States-(MPS)","page":"Chain ansatz","title":"Matrix Product States (MPS)","text":"","category":"section"},{"location":"ansatz/chain.html","page":"Chain ansatz","title":"Chain ansatz","text":"Matrix Product States (MPS) are a Quantum Tensor Network ansatz whose tensors are laid out in a 1D chain. Due to this, these networks are also known as Tensor Trains in other mathematical fields. Depending on the boundary conditions, the chains can be open or closed (i.e. periodic boundary conditions).","category":"page"},{"location":"ansatz/chain.html","page":"Chain ansatz","title":"Chain ansatz","text":"using Makie\nMakie.inline!(true)\nset_theme!(resolution=(800,200))\n\nusing CairoMakie\n\nusing Tenet\nusing NetworkLayout","category":"page"},{"location":"ansatz/chain.html","page":"Chain ansatz","title":"Chain ansatz","text":"fig = Figure() # hide\n\ntn_open = rand(MatrixProduct{State,Open}, n=10, χ=4) # hide\ntn_periodic = rand(MatrixProduct{State,Periodic}, n=10, χ=4) # hide\n\nplot!(fig[1,1], tn_open, layout=Spring(iterations=1000, C=0.5, seed=100)) # hide\nplot!(fig[1,2], tn_periodic, layout=Spring(iterations=1000, C=0.5, seed=100)) # hide\n\nLabel(fig[1,1, Bottom()], \"Open\") # hide\nLabel(fig[1,2, Bottom()], \"Periodic\") # hide\n\nfig # hide","category":"page"},{"location":"ansatz/chain.html#Matrix-Product-Operators-(MPO)","page":"Chain ansatz","title":"Matrix Product Operators (MPO)","text":"","category":"section"},{"location":"ansatz/chain.html","page":"Chain ansatz","title":"Chain ansatz","text":"Matrix Product Operators (MPO) are the operator version of Matrix Product State (MPS). The major difference between them is that MPOs have 2 indices per site (1 input and 1 output) while MPSs only have 1 index per site (i.e. an output).","category":"page"},{"location":"ansatz/chain.html","page":"Chain ansatz","title":"Chain ansatz","text":"fig = Figure() # hide\n\ntn_open = rand(MatrixProduct{Operator,Open}, n=10, χ=4) # hide\ntn_periodic = rand(MatrixProduct{Operator,Periodic}, n=10, χ=4) # hide\n\nplot!(fig[1,1], tn_open, layout=Spring(iterations=1000, C=0.5, seed=100)) # hide\nplot!(fig[1,2], tn_periodic, layout=Spring(iterations=1000, C=0.5, seed=100)) # hide\n\nLabel(fig[1,1, Bottom()], \"Open\") # hide\nLabel(fig[1,2, Bottom()], \"Periodic\") # hide\n\nfig # hide","category":"page"},{"location":"ansatz/chain.html","page":"Chain ansatz","title":"Chain ansatz","text":"In Tenet, the generic MatrixProduct ansatz implements this topology. Type variables are used to address their functionality (State or Operator) and their boundary conditions (Open or Periodic).","category":"page"},{"location":"ansatz/chain.html","page":"Chain ansatz","title":"Chain ansatz","text":"MatrixProduct\nMatrixProduct(::Any)","category":"page"},{"location":"alternatives.html#Alternatives","page":"Alternatives","title":"Alternatives","text":"","category":"section"},{"location":"alternatives.html","page":"Alternatives","title":"Alternatives","text":"Tenet is strongly opinionated. We acknowledge that it may not suit all cases (although we try 🙂). If your case doesn't fit Tenet's design, you can try the following libraries:","category":"page"},{"location":"alternatives.html","page":"Alternatives","title":"Alternatives","text":"quimb (Gray, 2018) Flexible Tensor Network written in Python. Main source of inspiration for Tenet.\ntenpy (Hauschild et al., 2021) Tensor Network library written in Python with a strong focus on physics.\nITensors.jl (Fishman et al., 2022) Mature Tensor Network framework written in Julia.\ntensorkrowch (Ramón Pareja Monturiol et al., 2023) A new Tensor Network library built on top of PyTorch.","category":"page"},{"location":"quantum.html#Quantum-Tensor-Networks","page":"Introduction","title":"Quantum Tensor Networks","text":"","category":"section"},{"location":"quantum.html","page":"Introduction","title":"Introduction","text":"Quantum\nTenet.TensorNetwork(::Quantum)\nBase.adjoint(::Quantum)\nsites\nnsites","category":"page"},{"location":"quantum.html#Tenet.Quantum","page":"Introduction","title":"Tenet.Quantum","text":"Quantum\n\nTensor Network with a notion of \"causality\". This leads to the notion of sites and directionality (input/output).\n\nNotes\n\nIndices are referenced by Sites.\n\n\n\n\n\n","category":"type"},{"location":"quantum.html#Tenet.TensorNetwork-Tuple{Quantum}","page":"Introduction","title":"Tenet.TensorNetwork","text":"TensorNetwork(q::AbstractQuantum)\n\nReturns the underlying TensorNetwork of an AbstractQuantum.\n\n\n\n\n\n","category":"method"},{"location":"quantum.html#Base.adjoint-Tuple{Quantum}","page":"Introduction","title":"Base.adjoint","text":"adjoint(q::Quantum)\n\nReturns the adjoint of a Quantum Tensor Network; i.e. the conjugate Tensor Network with the inputs and outputs swapped.\n\n\n\n\n\n","category":"method"},{"location":"quantum.html#Tenet.sites","page":"Introduction","title":"Tenet.sites","text":"sites(q::AbstractQuantum)\n\nReturns the sites of a AbstractQuantum Tensor Network.\n\n\n\n\n\n","category":"function"},{"location":"quantum.html#Tenet.nsites","page":"Introduction","title":"Tenet.nsites","text":"nsites(q::AbstractQuantum)\n\nReturns the number of sites of a AbstractQuantum Tensor Network.\n\n\n\n\n\n","category":"function"},{"location":"quantum.html#Queries","page":"Introduction","title":"Queries","text":"","category":"section"},{"location":"quantum.html","page":"Introduction","title":"Introduction","text":"Tenet.inds(::Quantum; kwargs...)\nTenet.tensors(::Quantum; kwargs...)","category":"page"},{"location":"quantum.html#Connecting-Quantum-Tensor-Networks","page":"Introduction","title":"Connecting Quantum Tensor Networks","text":"","category":"section"},{"location":"quantum.html","page":"Introduction","title":"Introduction","text":"inputs\noutputs\nlanes\nninputs\nnoutputs\nnlanes","category":"page"},{"location":"quantum.html","page":"Introduction","title":"Introduction","text":"Socket\nsocket(::Quantum)\nScalar\nState\nOperator","category":"page"},{"location":"quantum.html#Tenet.socket-Tuple{Quantum}","page":"Introduction","title":"Tenet.socket","text":"socket(q::Quantum)\n\nReturns the socket of a Quantum Tensor Network; i.e. whether it is a Scalar, State or Operator.\n\n\n\n\n\n","category":"method"},{"location":"quantum.html#Tenet.Scalar","page":"Introduction","title":"Tenet.Scalar","text":"Scalar <: Socket\n\nSocket representing a scalar; i.e. a Tensor Network with no open sites.\n\n\n\n\n\n","category":"type"},{"location":"quantum.html#Tenet.State","page":"Introduction","title":"Tenet.State","text":"State <: Socket\n\nSocket representing a state; i.e. a Tensor Network with only input sites (or only output sites if dual = true).\n\n\n\n\n\n","category":"type"},{"location":"quantum.html#Tenet.Operator","page":"Introduction","title":"Tenet.Operator","text":"Operator <: Socket\n\nSocket representing an operator; i.e. a Tensor Network with both input and output sites.\n\n\n\n\n\n","category":"type"},{"location":"quantum.html","page":"Introduction","title":"Introduction","text":"Base.merge(::Quantum, ::Quantum...)","category":"page"},{"location":"tensors.html#Tensors","page":"Tensors","title":"Tensors","text":"","category":"section"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"using Tenet","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"There are many jokes[1] about how to define a tensor. The definition we are giving here might not be the most correct one, but it is good enough for our use case (don't kill me please, mathematicians). A tensor T of order[2] n is a multilinear[3] application between n vector spaces over a field mathcalF.","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"[1]: For example, recursive definitions like a tensor is whatever that transforms as a tensor.","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"[2]: The order of a tensor may also be known as rank or dimensionality in other fields. However, these can be missleading, since it has nothing to do with the rank of linear algebra nor with the dimensionality of a vector space. We prefer to use word order.","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"[3]: Meaning that the relationships between the output and the inputs, and the inputs between them, are linear.","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"T  mathcalF^dim(1) times dots times mathcalF^dim(n) mapsto mathcalF","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"In layman's terms, it is a linear function whose inputs are vectors and the output is a scalar number.","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"T(mathbfv^(1) dots mathbfv^(n)) = c in mathcalF qquadqquad forall i mathbfv^(i) in mathcalF^dim(i)","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"Tensor algebra is a higher-order generalization of linear algebra, where scalar numbers can be viewed as order-0 tensors, vectors as order-1 tensors, matrices as order-2 tensors, ...","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"<img src=\"assets/tensor.excalidraw.svg\" class=\"invert-on-dark\"/>","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"Letters are used to identify each of the vector spaces the tensor relates to. In computer science, you would intuitively think of tensors as \"n-dimensional arrays with named dimensions\".","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"T_ijk iff mathttTijk","category":"page"},{"location":"tensors.html#The-Tensor-type","page":"Tensors","title":"The Tensor type","text":"","category":"section"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"In Tenet, a tensor is represented by the Tensor type, which wraps an array and a list of symbols. As it subtypes AbstractArray, many array operations can be dispatched to it.","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"You can create a Tensor by passing an array and a list of Symbols that name indices.","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"Tᵢⱼₖ = Tensor(rand(3,5,2), (:i,:j,:k))","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"The dimensionality or size of each index can be consulted using the size function.","category":"page"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"Base.size(::Tensor)","category":"page"},{"location":"tensors.html#Base.size-Tuple{Tensor}","page":"Tensors","title":"Base.size","text":"Base.size(::Tensor[, i])\n\nReturn the size of the underlying array or the dimension i (specified by Symbol or Integer).\n\n\n\n\n\n","category":"method"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"size(Tᵢⱼₖ)\nsize(Tᵢⱼₖ, :j)\nlength(Tᵢⱼₖ)","category":"page"},{"location":"tensors.html#Operations","page":"Tensors","title":"Operations","text":"","category":"section"},{"location":"tensors.html#Contraction","page":"Tensors","title":"Contraction","text":"","category":"section"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"Tenet.contract(::Tensor, ::Tensor)","category":"page"},{"location":"tensors.html#Graphs.LinAlg.contract-Tuple{Tensor, Tensor}","page":"Tensors","title":"Graphs.LinAlg.contract","text":"contract(a::Tensor[, b::Tensor]; dims=nonunique([inds(a)..., inds(b)...]))\n\nPerform tensor contraction operation.\n\n\n\n\n\n","category":"method"},{"location":"tensors.html#Factorizations","page":"Tensors","title":"Factorizations","text":"","category":"section"},{"location":"tensors.html","page":"Tensors","title":"Tensors","text":"LinearAlgebra.svd(::Tensor)\nLinearAlgebra.qr(::Tensor)\nLinearAlgebra.lu(::Tensor)","category":"page"},{"location":"tensors.html#LinearAlgebra.svd-Tuple{Tensor}","page":"Tensors","title":"LinearAlgebra.svd","text":"LinearAlgebra.svd(tensor::Tensor; left_inds, right_inds, virtualind, kwargs...)\n\nPerform SVD factorization on a tensor.\n\nKeyword arguments\n\nleft_inds: left indices to be used in the SVD factorization. Defaults to all indices of t except right_inds.\nright_inds: right indices to be used in the SVD factorization. Defaults to all indices of t except left_inds.\nvirtualind: name of the virtual bond. Defaults to a random Symbol.\n\n\n\n\n\n","category":"method"},{"location":"tensors.html#LinearAlgebra.qr-Tuple{Tensor}","page":"Tensors","title":"LinearAlgebra.qr","text":"LinearAlgebra.qr(tensor::Tensor; left_inds, right_inds, virtualind, kwargs...)\n\nPerform QR factorization on a tensor.\n\nKeyword arguments\n\nleft_inds: left indices to be used in the QR factorization. Defaults to all indices of t except right_inds.\nright_inds: right indices to be used in the QR factorization. Defaults to all indices of t except left_inds.\nvirtualind: name of the virtual bond. Defaults to a random Symbol.\n\n\n\n\n\n","category":"method"},{"location":"tensors.html#LinearAlgebra.lu-Tuple{Tensor}","page":"Tensors","title":"LinearAlgebra.lu","text":"LinearAlgebra.lu(tensor::Tensor; left_inds, right_inds, virtualind, kwargs...)\n\nPerform LU factorization on a tensor.\n\nKeyword arguments\n\nleft_inds: left indices to be used in the LU factorization. Defaults to all indices of t except right_inds.\nright_inds: right indices to be used in the LU factorization. Defaults to all indices of t except left_inds.\nvirtualind: name of the virtual bond. Defaults to a random Symbol.\n\n\n\n\n\n","category":"method"},{"location":"index.html#Tenet.jl","page":"Home","title":"Tenet.jl","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"info: BSC-Quantic's Registry\nTenet and some of its dependencies are located in our own Julia registry. In order to download Tenet, add our registry to your Julia installation by using the Pkg mode in a REPL session,using Pkg\npkg\"registry add https://github.com/bsc-quantic/Registry\"","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"A Julia library for Tensor Networks. Tenet can be executed both at local environments and on large supercomputers. Its goals are,","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Expressiveness Simple to use 👶\nFlexibility Extend it to your needs 🔧\nPerformance Goes brr... fast 🏎️","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"A video of its presentation at JuliaCon 2023 can be seen here:","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"<div class=\"youtube-video\">\n<iframe class=\"youtube-video\" width=\"560\" src=\"https://www.youtube-nocookie.com/embed/8BHGtm6FRMk?si=bPXB6bPtK695HFIR\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n</div>","category":"page"},{"location":"index.html#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Optimized Tensor Network contraction, powered by EinExprs\nTensor Network slicing/cuttings\nAutomatic Differentiation of TN contraction, powered by EinExprs and ChainRules\n3D visualization of large networks, powered by Makie","category":"page"},{"location":"tensor-network.html#Tensor-Networks","page":"Tensor Networks","title":"Tensor Networks","text":"","category":"section"},{"location":"tensor-network.html","page":"Tensor Networks","title":"Tensor Networks","text":"Tensor Networks (TN) are a graphical notation for representing complex multi-linear functions. For example, the following equation","category":"page"},{"location":"tensor-network.html","page":"Tensor Networks","title":"Tensor Networks","text":"sum_ijklmnop A_im B_ijp C_njk D_pkl E_mno F_ol","category":"page"},{"location":"tensor-network.html","page":"Tensor Networks","title":"Tensor Networks","text":"can be represented visually as","category":"page"},{"location":"tensor-network.html","page":"Tensor Networks","title":"Tensor Networks","text":"<figure>\n<img width=500 src=\"assets/tn-sketch.svg\" alt=\"Sketch of a Tensor Network\">\n<figcaption>Sketch of a Tensor Network</figcaption>\n</figure>","category":"page"},{"location":"tensor-network.html","page":"Tensor Networks","title":"Tensor Networks","text":"The graph's nodes represent tensors and edges represent tensor indices.","category":"page"},{"location":"tensor-network.html","page":"Tensor Networks","title":"Tensor Networks","text":"In Tenet, these objects are represented by the TensorNetwork type.","category":"page"},{"location":"tensor-network.html","page":"Tensor Networks","title":"Tensor Networks","text":"TensorNetwork","category":"page"},{"location":"tensor-network.html#Tenet.TensorNetwork","page":"Tensor Networks","title":"Tenet.TensorNetwork","text":"TensorNetwork\n\nGraph of interconnected tensors, representing a multilinear equation. Graph vertices represent tensors and graph edges, tensor indices.\n\n\n\n\n\n","category":"type"},{"location":"tensor-network.html","page":"Tensor Networks","title":"Tensor Networks","text":"Information about a TensorNetwork can be queried with the following functions.","category":"page"},{"location":"tensor-network.html#Query-information","page":"Tensor Networks","title":"Query information","text":"","category":"section"},{"location":"tensor-network.html","page":"Tensor Networks","title":"Tensor Networks","text":"inds(::Tenet.TensorNetwork)\nsize(::Tenet.TensorNetwork)\ntensors(::Tenet.TensorNetwork)","category":"page"},{"location":"tensor-network.html#Base.size-Tuple{TensorNetwork}","page":"Tensor Networks","title":"Base.size","text":"size(tn::AbstractTensorNetwork)\nsize(tn::AbstractTensorNetwork, index)\n\nReturn a mapping from indices to their dimensionalities.\n\nIf index is set, return the dimensionality of index. This is equivalent to size(tn)[index].\n\n\n\n\n\n","category":"method"},{"location":"tensor-network.html#Modification","page":"Tensor Networks","title":"Modification","text":"","category":"section"},{"location":"tensor-network.html#Add/Remove-tensors","page":"Tensor Networks","title":"Add/Remove tensors","text":"","category":"section"},{"location":"tensor-network.html","page":"Tensor Networks","title":"Tensor Networks","text":"push!(::Tenet.TensorNetwork, ::Tensor)\nappend!(::Tenet.TensorNetwork, ::Base.AbstractVecOrTuple{<:Tensor})\nmerge!(::Tenet.TensorNetwork, ::Tenet.TensorNetwork)\npop!(::Tenet.TensorNetwork, ::Tensor)\ndelete!(::Tenet.TensorNetwork, ::Any)","category":"page"},{"location":"tensor-network.html#Base.push!-Tuple{TensorNetwork, Tensor}","page":"Tensor Networks","title":"Base.push!","text":"push!(tn::AbstractTensorNetwork, tensor::Tensor)\n\nAdd a new tensor to the Tensor Network.\n\nSee also: append!, pop!.\n\n\n\n\n\n","category":"method"},{"location":"tensor-network.html#Base.append!-Tuple{TensorNetwork, Union{Tuple{Vararg{var\"#s12\"}}, AbstractVector{<:var\"#s12\"}} where var\"#s12\"<:Tensor}","page":"Tensor Networks","title":"Base.append!","text":"append!(tn::TensorNetwork, tensors::AbstractVecOrTuple{<:Tensor})\n\nAdd a list of tensors to a TensorNetwork.\n\nSee also: push!, merge!.\n\n\n\n\n\n","category":"method"},{"location":"tensor-network.html#Base.merge!-Tuple{TensorNetwork, TensorNetwork}","page":"Tensor Networks","title":"Base.merge!","text":"merge!(self::TensorNetwork, others::TensorNetwork...)\nmerge(self::TensorNetwork, others::TensorNetwork...)\n\nFuse various TensorNetworks into one.\n\nSee also: append!.\n\n\n\n\n\n","category":"method"},{"location":"tensor-network.html#Base.pop!-Tuple{TensorNetwork, Tensor}","page":"Tensor Networks","title":"Base.pop!","text":"pop!(tn::TensorNetwork, tensor::Tensor)\npop!(tn::TensorNetwork, i::Union{Symbol,AbstractVecOrTuple{Symbol}})\n\nRemove a tensor from the Tensor Network and returns it. If a Tensor is passed, then the first tensor satisfies egality (i.e. ≡ or ===) will be removed. If a Symbol or a list of Symbols is passed, then remove and return the tensors that contain all the indices.\n\nSee also: push!, delete!.\n\n\n\n\n\n","category":"method"},{"location":"tensor-network.html#Base.delete!-Tuple{TensorNetwork, Any}","page":"Tensor Networks","title":"Base.delete!","text":"delete!(tn::TensorNetwork, x)\n\nLike pop! but return the TensorNetwork instead.\n\n\n\n\n\n","category":"method"},{"location":"tensor-network.html#Replace-existing-elements","page":"Tensor Networks","title":"Replace existing elements","text":"","category":"section"},{"location":"tensor-network.html","page":"Tensor Networks","title":"Tensor Networks","text":"replace!","category":"page"},{"location":"tensor-network.html#Base.replace!","page":"Tensor Networks","title":"Base.replace!","text":"replace!(tn::AbstractTensorNetwork, old => new...)\nreplace(tn::AbstractTensorNetwork, old => new...)\n\nReplace the element in old with the one in new. Depending on the types of old and new, the following behaviour is expected:\n\nIf Symbols, it will correspond to a index renaming.\nIf Tensors, first element that satisfies egality (≡ or ===) will be replaced.\n\n\n\n\n\n","category":"function"},{"location":"tensor-network.html#Slicing","page":"Tensor Networks","title":"Slicing","text":"","category":"section"},{"location":"tensor-network.html","page":"Tensor Networks","title":"Tensor Networks","text":"selectdim\nslice!\nview(::Tenet.TensorNetwork)","category":"page"},{"location":"tensor-network.html#Base.selectdim","page":"Tensor Networks","title":"Base.selectdim","text":"selectdim(tn::AbstractTensorNetwork, index::Symbol, i)\n\nReturn a copy of the AbstractTensorNetwork where index has been projected to dimension i.\n\nSee also: view, slice!.\n\n\n\n\n\n","category":"function"},{"location":"tensor-network.html#Tenet.slice!","page":"Tensor Networks","title":"Tenet.slice!","text":"slice!(tn::AbstractTensorNetwork, index::Symbol, i)\n\nIn-place projection of index on dimension i.\n\nSee also: selectdim, view.\n\n\n\n\n\n","category":"function"},{"location":"tensor-network.html#Base.view-Tuple{TensorNetwork}","page":"Tensor Networks","title":"Base.view","text":"view(tn::AbstractTensorNetwork, index => i...)\n\nReturn a copy of the AbstractTensorNetwork where each index has been projected to dimension i. It is equivalent to a recursive call of selectdim.\n\nSee also: selectdim, slice!.\n\n\n\n\n\n","category":"method"},{"location":"tensor-network.html#Miscelaneous","page":"Tensor Networks","title":"Miscelaneous","text":"","category":"section"},{"location":"tensor-network.html","page":"Tensor Networks","title":"Tensor Networks","text":"Base.copy(::Tenet.TensorNetwork)\nBase.rand(::Type{TensorNetwork}, n::Integer, regularity::Integer)","category":"page"},{"location":"tensor-network.html#Base.copy-Tuple{TensorNetwork}","page":"Tensor Networks","title":"Base.copy","text":"copy(tn::TensorNetwork)\n\nReturn a shallow copy of a TensorNetwork.\n\n\n\n\n\n","category":"method"}]
}
