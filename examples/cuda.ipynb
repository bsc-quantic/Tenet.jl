{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA tensor network contraction demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "- The system must have a CUDA GPU available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Tenet\n",
    "using EinExprs\n",
    "using Adapt\n",
    "using CUDA\n",
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a random tensor network and find its contraction path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EinExpr(Symbol[], EinExpr[EinExpr([:b, :Y, :d, :M, :S, :X, :J, :V, :B], EinExpr[EinExpr([:H, :b, :Y, :c, :d, :M, :S], EinExpr[EinExpr([:a, :P, :H, :b, :E, :Z], EinExpr[EinExpr([:L, :a, :P, :H, :W], EinExpr[], Dict(:a => 3, :P => 6, :H => 6, :W => 3, :L => 8)), EinExpr([:L, :b, :E, :Z, :W], EinExpr[], Dict(:Z => 3, :b => 3, :W => 3, :L => 8, :E => 2))], Dict{Symbol, Int64}()), EinExpr([:Y, :E, :P, :a, :c, :d, :M, :Z, :S], EinExpr[EinExpr([:U, :R, :Y, :E, :D, :P], EinExpr[], Dict(:U => 3, :P => 6, :R => 6, :D => 3, :Y => 9, :E => 2)), EinExpr([:a, :R, :D, :c, :d, :M, :Z, :S, :U], EinExpr[], Dict(:a => 3, :Z => 3, :R => 6, :M => 9, :d => 3, :U => 3, :D => 3, :c => 5, :S => 3))], Dict{Symbol, Int64}())], Dict{Symbol, Int64}()), EinExpr([:X, :J, :H, :V, :c, :B], EinExpr[EinExpr([:A, :X, :J, :K, :H], EinExpr[], Dict(:A => 6, :J => 2, :H => 6, :K => 4, :X => 8)), EinExpr([:V, :c, :A, :K, :B], EinExpr[], Dict(:A => 6, :K => 4, :B => 5, :V => 8, :c => 5))], Dict{Symbol, Int64}())], Dict{Symbol, Int64}()), EinExpr([:b, :M, :J, :B, :S, :X, :Y, :V, :d], EinExpr[EinExpr([:C, :b, :F, :M, :I, :J, :B], EinExpr[EinExpr([:C, :O, :T, :b, :G, :N], EinExpr[EinExpr([:Q, :C], EinExpr[], Dict(:Q => 9, :C => 5)), EinExpr([:Q, :O, :T, :b, :G, :N], EinExpr[], Dict(:T => 3, :b => 3, :N => 3, :G => 9, :Q => 9, :O => 9))], Dict{Symbol, Int64}()), EinExpr([:F, :M, :I, :G, :O, :J, :T, :B, :N], EinExpr[], Dict(:T => 3, :I => 4, :F => 5, :M => 9, :G => 9, :J => 2, :N => 3, :B => 5, :O => 9))], Dict{Symbol, Int64}()), EinExpr([:C, :I, :S, :X, :F, :Y, :V, :d], EinExpr[], Dict(:I => 4, :F => 5, :d => 3, :S => 3, :X => 8, :Y => 9, :V => 8, :C => 5))], Dict{Symbol, Int64}())], Dict{Symbol, Int64}())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize random tensor network\n",
    "regularity = 6\n",
    "ntensors = 10\n",
    "tn = rand(TensorNetwork, ntensors, regularity)\n",
    "path = einexpr(tn; optimizer=Exhaustive())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the tensors' data types to `CuArray`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "CUDA driver not found",
     "output_type": "error",
     "traceback": [
      "CUDA driver not found\n",
      "\n",
      "Stacktrace:\n",
      "  [1] error(s::String)\n",
      "    @ Base ./error.jl:35\n",
      "  [2] functional\n",
      "    @ CUDA ~/.julia/packages/CUDA/75aiI/src/initialization.jl:24 [inlined]\n",
      "  [3] task_local_state!()\n",
      "    @ CUDA ~/.julia/packages/CUDA/75aiI/lib/cudadrv/state.jl:77\n",
      "  [4] device\n",
      "    @ CUDA ~/.julia/packages/CUDA/75aiI/lib/cudadrv/state.jl:189 [inlined]\n",
      "  [5] memory_stats\n",
      "    @ CUDA ~/.julia/packages/CUDA/75aiI/src/memory.jl:58 [inlined]\n",
      "  [6] maybe_collect(will_block::Bool)\n",
      "    @ CUDA ~/.julia/packages/CUDA/75aiI/src/memory.jl:69\n",
      "  [7] maybe_collect(will_block::Bool)\n",
      "    @ CUDA ~/.julia/packages/CUDA/75aiI/src/memory.jl:65 [inlined]\n",
      "  [8] pool_alloc\n",
      "    @ ~/.julia/packages/CUDA/75aiI/src/memory.jl:615 [inlined]\n",
      "  [9] CuArray{Float64, 5, CUDA.DeviceMemory}(::UndefInitializer, dims::NTuple{5, Int64})\n",
      "    @ CUDA ~/.julia/packages/CUDA/75aiI/src/array.jl:74\n",
      " [10] CuArray{T, N, M}(::UndefInitializer, dims::Tuple{Vararg{Int64, N}}) where {T, N, M}\n",
      "    @ CUDA ~/.julia/packages/CUDA/75aiI/src/array.jl:405 [inlined]\n",
      " [11] CuArray{T, N, M}(::UndefInitializer, dims::Tuple{Vararg{Int64, N}}) where {T, N, M}\n",
      "    @ CUDA ~/.julia/packages/CUDA/75aiI/src/array.jl:410 [inlined]\n",
      " [12] CuArray(A::Array{Float64, 5})\n",
      "    @ CUDA ~/.julia/packages/CUDA/75aiI/src/array.jl:419\n",
      " [13] convert\n",
      "    @ GPUArrays ~/.julia/packages/GPUArrays/HjWFN/src/host/construction.jl:4 [inlined]\n",
      " [14] adapt_storage(::Type{CuArray}, xs::Array{Float64, 5})\n",
      "    @ CUDA ~/.julia/packages/CUDA/75aiI/src/array.jl:677\n",
      " [15] adapt_structure(to::Type, x::Array{Float64, 5})\n",
      "    @ Adapt ~/.julia/packages/Adapt/7T9au/src/Adapt.jl:57\n",
      " [16] adapt\n",
      "    @ ~/.julia/packages/Adapt/7T9au/src/Adapt.jl:40 [inlined]\n",
      " [17] adapt_structure(to::Type, x::Tensor{Float64, 5, Array{Float64, 5}})\n",
      "    @ TenetAdaptExt ~/QuanticGit/Tenet.jl/ext/TenetAdaptExt.jl:6\n",
      " [18] adapt(to::Type, x::Tensor{Float64, 5, Array{Float64, 5}})\n",
      "    @ Adapt ~/.julia/packages/Adapt/7T9au/src/Adapt.jl:40\n",
      " [19] _broadcast_getindex_evalf\n",
      "    @ Base.Broadcast ./broadcast.jl:709 [inlined]\n",
      " [20] _broadcast_getindex\n",
      "    @ Base.Broadcast ./broadcast.jl:682 [inlined]\n",
      " [21] getindex\n",
      "    @ Base.Broadcast ./broadcast.jl:636 [inlined]\n",
      " [22] copy\n",
      "    @ Base.Broadcast ./broadcast.jl:942 [inlined]\n",
      " [23] materialize(bc::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1}, Nothing, typeof(adapt), Tuple{Base.RefValue{UnionAll}, Vector{Tensor}}})\n",
      "    @ Base.Broadcast ./broadcast.jl:903\n",
      " [24] adapt_structure(to::Type, x::TensorNetwork)\n",
      "    @ TenetAdaptExt ~/QuanticGit/Tenet.jl/ext/TenetAdaptExt.jl:8\n",
      " [25] adapt(to::Type, x::TensorNetwork)\n",
      "    @ Adapt ~/.julia/packages/Adapt/7T9au/src/Adapt.jl:40\n",
      " [26] top-level scope\n",
      "    @ ~/QuanticGit/Tenet.jl/examples/cuda.ipynb:1"
     ]
    }
   ],
   "source": [
    "cudatn = adapt(CuArray, tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark CUDA tensor network contraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `cudatn` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `cudatn` not defined\n",
      "\n",
      "Stacktrace:\n",
      "  [1] macro expansion\n",
      "    @ ~/.julia/packages/CUDA/75aiI/src/utilities.jl:35 [inlined]\n",
      "  [2] var\"##core#226\"()\n",
      "    @ Main ~/.julia/packages/BenchmarkTools/QNsku/src/execution.jl:561\n",
      "  [3] var\"##sample#227\"(::Tuple{}, __params::BenchmarkTools.Parameters)\n",
      "    @ Main ~/.julia/packages/BenchmarkTools/QNsku/src/execution.jl:570\n",
      "  [4] _lineartrial(b::BenchmarkTools.Benchmark, p::BenchmarkTools.Parameters; maxevals::Int64, kwargs::@Kwargs{})\n",
      "    @ BenchmarkTools ~/.julia/packages/BenchmarkTools/QNsku/src/execution.jl:187\n",
      "  [5] _lineartrial(b::BenchmarkTools.Benchmark, p::BenchmarkTools.Parameters)\n",
      "    @ BenchmarkTools ~/.julia/packages/BenchmarkTools/QNsku/src/execution.jl:182\n",
      "  [6] #invokelatest#2\n",
      "    @ ./essentials.jl:887 [inlined]\n",
      "  [7] invokelatest\n",
      "    @ ./essentials.jl:884 [inlined]\n",
      "  [8] #lineartrial#46\n",
      "    @ ~/.julia/packages/BenchmarkTools/QNsku/src/execution.jl:51 [inlined]\n",
      "  [9] lineartrial\n",
      "    @ ~/.julia/packages/BenchmarkTools/QNsku/src/execution.jl:50 [inlined]\n",
      " [10] tune!(b::BenchmarkTools.Benchmark, p::BenchmarkTools.Parameters; progressid::Nothing, nleaves::Float64, ndone::Float64, verbose::Bool, pad::String, kwargs::@Kwargs{})\n",
      "    @ BenchmarkTools ~/.julia/packages/BenchmarkTools/QNsku/src/execution.jl:300\n",
      " [11] tune!\n",
      "    @ BenchmarkTools ~/.julia/packages/BenchmarkTools/QNsku/src/execution.jl:289 [inlined]\n",
      " [12] tune!(b::BenchmarkTools.Benchmark)\n",
      "    @ BenchmarkTools ~/.julia/packages/BenchmarkTools/QNsku/src/execution.jl:289\n",
      " [13] top-level scope\n",
      "    @ ~/.julia/packages/BenchmarkTools/QNsku/src/execution.jl:447"
     ]
    }
   ],
   "source": [
    "@benchmark CUDA.@sync contract(cudatn; path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark regular tensor network contraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "CUDA driver not found",
     "output_type": "error",
     "traceback": [
      "CUDA driver not found\n",
      "\n",
      "Stacktrace:\n",
      "  [1] error(s::String)\n",
      "    @ Base ./error.jl:35\n",
      "  [2] functional\n",
      "    @ CUDA ~/.julia/packages/CUDA/75aiI/src/initialization.jl:24 [inlined]\n",
      "  [3] task_local_state!()\n",
      "    @ CUDA ~/.julia/packages/CUDA/75aiI/lib/cudadrv/state.jl:77\n",
      "  [4] stream\n",
      "    @ ~/.julia/packages/CUDA/75aiI/lib/cudadrv/state.jl:370 [inlined]\n",
      "  [5] synchronize\n",
      "    @ ~/.julia/packages/CUDA/75aiI/lib/cudadrv/synchronization.jl:196 [inlined]\n",
      "  [6] macro expansion\n",
      "    @ ~/.julia/packages/CUDA/75aiI/src/utilities.jl:36 [inlined]\n",
      "  [7] var\"##core#229\"()\n",
      "    @ Main ~/.julia/packages/BenchmarkTools/QNsku/src/execution.jl:561\n",
      "  [8] var\"##sample#230\"(::Tuple{}, __params::BenchmarkTools.Parameters)\n",
      "    @ Main ~/.julia/packages/BenchmarkTools/QNsku/src/execution.jl:570\n",
      "  [9] _lineartrial(b::BenchmarkTools.Benchmark, p::BenchmarkTools.Parameters; maxevals::Int64, kwargs::@Kwargs{})\n",
      "    @ BenchmarkTools ~/.julia/packages/BenchmarkTools/QNsku/src/execution.jl:187\n",
      " [10] _lineartrial(b::BenchmarkTools.Benchmark, p::BenchmarkTools.Parameters)\n",
      "    @ BenchmarkTools ~/.julia/packages/BenchmarkTools/QNsku/src/execution.jl:182\n",
      " [11] #invokelatest#2\n",
      "    @ ./essentials.jl:887 [inlined]\n",
      " [12] invokelatest\n",
      "    @ ./essentials.jl:884 [inlined]\n",
      " [13] #lineartrial#46\n",
      "    @ ~/.julia/packages/BenchmarkTools/QNsku/src/execution.jl:51 [inlined]\n",
      " [14] lineartrial\n",
      "    @ ~/.julia/packages/BenchmarkTools/QNsku/src/execution.jl:50 [inlined]\n",
      " [15] tune!(b::BenchmarkTools.Benchmark, p::BenchmarkTools.Parameters; progressid::Nothing, nleaves::Float64, ndone::Float64, verbose::Bool, pad::String, kwargs::@Kwargs{})\n",
      "    @ BenchmarkTools ~/.julia/packages/BenchmarkTools/QNsku/src/execution.jl:300\n",
      " [16] tune!\n",
      "    @ BenchmarkTools ~/.julia/packages/BenchmarkTools/QNsku/src/execution.jl:289 [inlined]\n",
      " [17] tune!(b::BenchmarkTools.Benchmark)\n",
      "    @ BenchmarkTools ~/.julia/packages/BenchmarkTools/QNsku/src/execution.jl:289\n",
      " [18] top-level scope\n",
      "    @ ~/.julia/packages/BenchmarkTools/QNsku/src/execution.jl:447"
     ]
    }
   ],
   "source": [
    "@benchmark CUDA.@sync contract(tn; path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
